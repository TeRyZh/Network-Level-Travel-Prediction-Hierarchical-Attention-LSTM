{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked_LSTM_TensorFlow.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import os\n"
      ],
      "metadata": {
        "id": "uhDVoC-pLugO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTFk-bGPwjM4",
        "outputId": "67b8e513-fbbc-4e4e-9d5c-b48e3552e906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4O_AyUwXA7",
        "outputId": "d7fde176-5057-4d3d-e184-b73e4244d440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 14 04:04:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #1: Preprocessing the Dataset for Time Series Analysis"
      ],
      "metadata": {
        "id": "wqlxS-djLTLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "travel_time_df = pd.read_csv('/content/drive/MyDrive/CIS 545 Project Folder/Data Set/2021_travel_time.csv')\n",
        "corridor_file = \"/content/drive/MyDrive/CIS 545 Project Folder/Data Set/pems_district4_corridors.csv\"\n",
        "corridor_df = pd.read_csv(corridor_file)\n",
        "\n",
        "travel_time_df = travel_time_df[travel_time_df['5 Minutes'] > '2021-1-01 00:00:00']"
      ],
      "metadata": {
        "id": "FEvqRQwEyUkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corridors_to_merge = []\n",
        "\n",
        "for row_num, corridor in corridor_df.iterrows():  \t\n",
        "      if row_num == 0:\n",
        "          all_corridors_tt_2021 = travel_time_df[(travel_time_df['corridor_name'] == corridor[\"Corridor\"])\t&  (travel_time_df['direction'] == corridor[\"Fwy-Dir\"])][[\"5 Minutes\",\t\"Mainline Agg\"]]\n",
        "      else:\n",
        "          corridor_tt_2021 = travel_time_df[(travel_time_df['corridor_name'] == corridor[\"Corridor\"])\t&  (travel_time_df['direction'] == corridor[\"Fwy-Dir\"])][[\"5 Minutes\",\t\"Mainline Agg\"]]\n",
        "          all_corridors_tt_2021 = all_corridors_tt_2021.merge(corridor_tt_2021, \"outer\", left_on='5 Minutes', right_on='5 Minutes')"
      ],
      "metadata": {
        "id": "yAU2fJXWyXwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_corridors_tt_2021 = all_corridors_tt_2021.dropna(axis=1)\n",
        "all_corridors_tt_2021"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "YUXB84hkyagy",
        "outputId": "7d384f71-43f0-4765-a5d7-cb4f8f2d5056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-15c23b08-74be-4477-b56a-ef83b62ab9c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5 Minutes</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.90</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.4</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.90</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.4</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.90</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.4</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.90</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.4</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.90</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.4</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55867</th>\n",
              "      <td>2021-10-31 23:55:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>5.33</td>\n",
              "      <td>10.78</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>12.98</td>\n",
              "      <td>14.45</td>\n",
              "      <td>6.17</td>\n",
              "      <td>7.58</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.42</td>\n",
              "      <td>11.87</td>\n",
              "      <td>11.78</td>\n",
              "      <td>12.05</td>\n",
              "      <td>7.77</td>\n",
              "      <td>17.98</td>\n",
              "      <td>15.05</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.25</td>\n",
              "      <td>12.42</td>\n",
              "      <td>12.20</td>\n",
              "      <td>8.03</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.18</td>\n",
              "      <td>12.40</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.7</td>\n",
              "      <td>25.53</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55868</th>\n",
              "      <td>2021-10-31 23:55:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>5.33</td>\n",
              "      <td>10.78</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>12.98</td>\n",
              "      <td>14.45</td>\n",
              "      <td>6.17</td>\n",
              "      <td>7.58</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.42</td>\n",
              "      <td>11.87</td>\n",
              "      <td>11.78</td>\n",
              "      <td>12.05</td>\n",
              "      <td>7.77</td>\n",
              "      <td>17.98</td>\n",
              "      <td>15.05</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.25</td>\n",
              "      <td>12.42</td>\n",
              "      <td>12.20</td>\n",
              "      <td>8.03</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.18</td>\n",
              "      <td>12.40</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.7</td>\n",
              "      <td>25.53</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55869</th>\n",
              "      <td>2021-10-31 23:55:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>5.33</td>\n",
              "      <td>10.78</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>12.98</td>\n",
              "      <td>14.45</td>\n",
              "      <td>6.17</td>\n",
              "      <td>7.58</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.42</td>\n",
              "      <td>11.87</td>\n",
              "      <td>11.78</td>\n",
              "      <td>12.05</td>\n",
              "      <td>7.77</td>\n",
              "      <td>17.98</td>\n",
              "      <td>15.05</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.25</td>\n",
              "      <td>12.42</td>\n",
              "      <td>12.20</td>\n",
              "      <td>8.03</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.18</td>\n",
              "      <td>12.40</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.7</td>\n",
              "      <td>25.53</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55870</th>\n",
              "      <td>2021-10-31 23:55:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>5.33</td>\n",
              "      <td>10.78</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>12.98</td>\n",
              "      <td>14.45</td>\n",
              "      <td>6.17</td>\n",
              "      <td>7.58</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.42</td>\n",
              "      <td>11.87</td>\n",
              "      <td>11.78</td>\n",
              "      <td>12.05</td>\n",
              "      <td>7.77</td>\n",
              "      <td>17.98</td>\n",
              "      <td>15.05</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.25</td>\n",
              "      <td>12.42</td>\n",
              "      <td>12.20</td>\n",
              "      <td>8.03</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.18</td>\n",
              "      <td>12.40</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.7</td>\n",
              "      <td>25.53</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55871</th>\n",
              "      <td>2021-10-31 23:55:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>5.33</td>\n",
              "      <td>10.78</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>12.98</td>\n",
              "      <td>14.45</td>\n",
              "      <td>6.17</td>\n",
              "      <td>7.58</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.42</td>\n",
              "      <td>11.87</td>\n",
              "      <td>11.78</td>\n",
              "      <td>12.05</td>\n",
              "      <td>7.77</td>\n",
              "      <td>17.98</td>\n",
              "      <td>15.05</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.25</td>\n",
              "      <td>12.42</td>\n",
              "      <td>12.20</td>\n",
              "      <td>8.03</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.18</td>\n",
              "      <td>12.40</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.7</td>\n",
              "      <td>25.53</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55872 rows Ã— 42 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15c23b08-74be-4477-b56a-ef83b62ab9c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15c23b08-74be-4477-b56a-ef83b62ab9c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15c23b08-74be-4477-b56a-ef83b62ab9c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 5 Minutes  Mainline Agg_x  ...  Mainline Agg_x  Mainline Agg_y\n",
              "0      2021-10-01 00:00:00           10.90  ...           15.75           15.78\n",
              "1      2021-10-01 00:00:00           10.90  ...           15.75           15.78\n",
              "2      2021-10-01 00:00:00           10.90  ...           15.75           15.78\n",
              "3      2021-10-01 00:00:00           10.90  ...           15.75           15.78\n",
              "4      2021-10-01 00:00:00           10.90  ...           15.75           15.78\n",
              "...                    ...             ...  ...             ...             ...\n",
              "55867  2021-10-31 23:55:00           10.88  ...           15.70           15.72\n",
              "55868  2021-10-31 23:55:00           10.88  ...           15.70           15.72\n",
              "55869  2021-10-31 23:55:00           10.88  ...           15.70           15.72\n",
              "55870  2021-10-31 23:55:00           10.88  ...           15.70           15.72\n",
              "55871  2021-10-31 23:55:00           10.88  ...           15.70           15.72\n",
              "\n",
              "[55872 rows x 42 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_columns = [f\"travel_time_{n}\" for n in range(len(all_corridors_tt_2021.columns) - 1)]\n",
        "new_columns.insert(0, \"timestamp\")\n",
        "# len(new_columns)\n",
        "all_corridors_tt_2021 = all_corridors_tt_2021.set_axis(new_columns, axis=1, inplace=False)\n",
        "num_records, num_corridors = all_corridors_tt_2021.shape"
      ],
      "metadata": {
        "id": "CePIcZwVydYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_corridors_tt_2021.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "__CUvv1eyfC8",
        "outputId": "13b74dad-692f-48bc-b4da-d72f20e8b87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4fc15709-2caf-4fa0-94ea-eef2eb95e204\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>travel_time_0</th>\n",
              "      <th>travel_time_1</th>\n",
              "      <th>travel_time_2</th>\n",
              "      <th>travel_time_3</th>\n",
              "      <th>travel_time_4</th>\n",
              "      <th>travel_time_5</th>\n",
              "      <th>travel_time_6</th>\n",
              "      <th>travel_time_7</th>\n",
              "      <th>travel_time_8</th>\n",
              "      <th>travel_time_9</th>\n",
              "      <th>travel_time_10</th>\n",
              "      <th>travel_time_11</th>\n",
              "      <th>travel_time_12</th>\n",
              "      <th>travel_time_13</th>\n",
              "      <th>travel_time_14</th>\n",
              "      <th>travel_time_15</th>\n",
              "      <th>travel_time_16</th>\n",
              "      <th>travel_time_17</th>\n",
              "      <th>travel_time_18</th>\n",
              "      <th>travel_time_19</th>\n",
              "      <th>travel_time_20</th>\n",
              "      <th>travel_time_21</th>\n",
              "      <th>travel_time_22</th>\n",
              "      <th>travel_time_23</th>\n",
              "      <th>travel_time_24</th>\n",
              "      <th>travel_time_25</th>\n",
              "      <th>travel_time_26</th>\n",
              "      <th>travel_time_27</th>\n",
              "      <th>travel_time_28</th>\n",
              "      <th>travel_time_29</th>\n",
              "      <th>travel_time_30</th>\n",
              "      <th>travel_time_31</th>\n",
              "      <th>travel_time_32</th>\n",
              "      <th>travel_time_33</th>\n",
              "      <th>travel_time_34</th>\n",
              "      <th>travel_time_35</th>\n",
              "      <th>travel_time_36</th>\n",
              "      <th>travel_time_37</th>\n",
              "      <th>travel_time_38</th>\n",
              "      <th>travel_time_39</th>\n",
              "      <th>travel_time_40</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-10-01 00:00:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>10.72</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.45</td>\n",
              "      <td>13.08</td>\n",
              "      <td>6.43</td>\n",
              "      <td>6.12</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.52</td>\n",
              "      <td>27.58</td>\n",
              "      <td>11.95</td>\n",
              "      <td>11.78</td>\n",
              "      <td>11.98</td>\n",
              "      <td>7.55</td>\n",
              "      <td>17.87</td>\n",
              "      <td>14.87</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.28</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.28</td>\n",
              "      <td>8.08</td>\n",
              "      <td>8.15</td>\n",
              "      <td>17.15</td>\n",
              "      <td>12.18</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.32</td>\n",
              "      <td>18.77</td>\n",
              "      <td>25.40</td>\n",
              "      <td>25.52</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2021-10-01 00:05:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.58</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.62</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.23</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.25</td>\n",
              "      <td>13.38</td>\n",
              "      <td>13.05</td>\n",
              "      <td>6.47</td>\n",
              "      <td>6.08</td>\n",
              "      <td>4.22</td>\n",
              "      <td>4.63</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.57</td>\n",
              "      <td>27.75</td>\n",
              "      <td>11.93</td>\n",
              "      <td>11.98</td>\n",
              "      <td>12.02</td>\n",
              "      <td>7.62</td>\n",
              "      <td>17.98</td>\n",
              "      <td>14.83</td>\n",
              "      <td>3.22</td>\n",
              "      <td>3.30</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.33</td>\n",
              "      <td>8.28</td>\n",
              "      <td>8.20</td>\n",
              "      <td>17.13</td>\n",
              "      <td>12.17</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.33</td>\n",
              "      <td>18.78</td>\n",
              "      <td>25.48</td>\n",
              "      <td>25.58</td>\n",
              "      <td>5.75</td>\n",
              "      <td>6.08</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2021-10-01 00:05:00</td>\n",
              "      <td>10.9</td>\n",
              "      <td>11.58</td>\n",
              "      <td>5.32</td>\n",
              "      <td>10.62</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.23</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.25</td>\n",
              "      <td>13.38</td>\n",
              "      <td>13.05</td>\n",
              "      <td>6.47</td>\n",
              "      <td>6.08</td>\n",
              "      <td>4.22</td>\n",
              "      <td>4.63</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.57</td>\n",
              "      <td>27.75</td>\n",
              "      <td>11.93</td>\n",
              "      <td>11.98</td>\n",
              "      <td>12.02</td>\n",
              "      <td>7.62</td>\n",
              "      <td>17.98</td>\n",
              "      <td>14.83</td>\n",
              "      <td>3.22</td>\n",
              "      <td>3.30</td>\n",
              "      <td>12.43</td>\n",
              "      <td>12.33</td>\n",
              "      <td>8.28</td>\n",
              "      <td>8.20</td>\n",
              "      <td>17.13</td>\n",
              "      <td>12.17</td>\n",
              "      <td>13.92</td>\n",
              "      <td>13.93</td>\n",
              "      <td>18.33</td>\n",
              "      <td>18.78</td>\n",
              "      <td>25.48</td>\n",
              "      <td>25.58</td>\n",
              "      <td>5.75</td>\n",
              "      <td>6.08</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fc15709-2caf-4fa0-94ea-eef2eb95e204')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4fc15709-2caf-4fa0-94ea-eef2eb95e204 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4fc15709-2caf-4fa0-94ea-eef2eb95e204');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             timestamp  travel_time_0  ...  travel_time_39  travel_time_40\n",
              "0  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "1  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "2  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "3  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "4  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "5  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "6  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "7  2021-10-01 00:00:00           10.9  ...           15.75           15.78\n",
              "8  2021-10-01 00:05:00           10.9  ...           15.70           15.80\n",
              "9  2021-10-01 00:05:00           10.9  ...           15.70           15.80\n",
              "\n",
              "[10 rows x 42 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset_many_to_one(array,time_steps, num_sample, Horizon = 0):\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    indices = random.sample(range(time_steps, len(array) - Horizon), num_sample)\n",
        "\n",
        "    for i in indices:\n",
        "        x.append(array[i-time_steps:i])\n",
        "        y.append(array[i + Horizon])\n",
        "        \n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# train_data = all_corridors_tt_2021.drop(['timestamp'], axis=1).iloc[:num_training]\n",
        "total_data = all_corridors_tt_2021.drop(['timestamp'], axis=1)\n",
        " #for testing at bottom\n",
        "print(total_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdF7_dhzwTVO",
        "outputId": "3cf21fa2-5e22-48fd-fdcd-539b8d4c2a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       travel_time_0  travel_time_1  ...  travel_time_39  travel_time_40\n",
            "0              10.90          11.55  ...           15.75           15.78\n",
            "1              10.90          11.55  ...           15.75           15.78\n",
            "2              10.90          11.55  ...           15.75           15.78\n",
            "3              10.90          11.55  ...           15.75           15.78\n",
            "4              10.90          11.55  ...           15.75           15.78\n",
            "...              ...            ...  ...             ...             ...\n",
            "55867          10.88          11.60  ...           15.70           15.72\n",
            "55868          10.88          11.60  ...           15.70           15.72\n",
            "55869          10.88          11.60  ...           15.70           15.72\n",
            "55870          10.88          11.60  ...           15.70           15.72\n",
            "55871          10.88          11.60  ...           15.70           15.72\n",
            "\n",
            "[55872 rows x 41 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#scale values to [0,1]\n",
        "num_training = 16000\n",
        "num_validate = 4000\n",
        "num_test = 4000\n",
        "time_steps = 24  # 2 hours\n",
        "num_corridor = total_data.shape[1]\n",
        "hidden_size = 120\n",
        "nfc = 512            # fully connected layer for attention\n",
        "drop_prob = 0.5\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaler.fit_transform(total_data)\n",
        "# train = scaler.transform(train_data)\n",
        "total = scaler.transform(total_data)\n",
        "\n",
        "sc = MinMaxScaler()\n",
        "total_data = sc.fit_transform(total_data)\n",
        "\n",
        "## 15 minutes horizon\n",
        "trn_x, trn_y = make_dataset_many_to_one(total, time_steps, num_training)  \n",
        "vld_x, vld_y = make_dataset_many_to_one(total, time_steps, num_validate)  \n",
        "tst_x, tst_y = make_dataset_many_to_one(total, time_steps, num_test)  \n",
        "\n",
        "trn_x = trn_x.reshape(-1, time_steps, num_corridor)\n",
        "vld_x = vld_x.reshape(-1, time_steps, num_corridor)\n",
        "tst_x = tst_x.reshape(-1, time_steps, num_corridor)"
      ],
      "metadata": {
        "id": "IQujqRj2wQtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and fit the LSTM network, optimizer=adam, 25 neurons, dropout 0.1\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, return_sequences=True, input_shape=(time_steps, num_corridor))) # returns a sequence of vectors of dimension 32\n",
        "model.add(LSTM(hidden_size))  \n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(nfc))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(num_corridor))\n",
        "\n",
        "start = time.time()\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "history = model.fit(trn_x, trn_y, validation_split = 0.25, epochs=1000, batch_size=50, verbose=1)\n",
        "\n",
        "end = time.time()\n",
        "print('time %.2f sec' % (end-start))"
      ],
      "metadata": {
        "id": "RN2ZHrIYLnB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc024bdd-bd7d-4c29-ba16-a043721b11ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "240/240 [==============================] - 6s 11ms/step - loss: 0.0014 - val_loss: 4.6524e-04\n",
            "Epoch 2/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 6.3304e-04 - val_loss: 3.6041e-04\n",
            "Epoch 3/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 4.9870e-04 - val_loss: 2.7777e-04\n",
            "Epoch 4/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 4.5261e-04 - val_loss: 2.5761e-04\n",
            "Epoch 5/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 4.2721e-04 - val_loss: 2.1746e-04\n",
            "Epoch 6/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.9320e-04 - val_loss: 2.1820e-04\n",
            "Epoch 7/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.8462e-04 - val_loss: 1.7690e-04\n",
            "Epoch 8/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 3.7517e-04 - val_loss: 1.7518e-04\n",
            "Epoch 9/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 3.5854e-04 - val_loss: 2.0166e-04\n",
            "Epoch 10/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.4947e-04 - val_loss: 1.8127e-04\n",
            "Epoch 11/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.3860e-04 - val_loss: 1.6790e-04\n",
            "Epoch 12/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.3520e-04 - val_loss: 1.6488e-04\n",
            "Epoch 13/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 3.3399e-04 - val_loss: 1.6645e-04\n",
            "Epoch 14/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.2734e-04 - val_loss: 1.5520e-04\n",
            "Epoch 15/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 3.1808e-04 - val_loss: 1.5359e-04\n",
            "Epoch 16/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.2287e-04 - val_loss: 1.5694e-04\n",
            "Epoch 17/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.0789e-04 - val_loss: 1.3029e-04\n",
            "Epoch 18/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.1545e-04 - val_loss: 1.4374e-04\n",
            "Epoch 19/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.0415e-04 - val_loss: 1.4177e-04\n",
            "Epoch 20/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 3.0488e-04 - val_loss: 1.4652e-04\n",
            "Epoch 21/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.0782e-04 - val_loss: 1.4208e-04\n",
            "Epoch 22/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 3.0948e-04 - val_loss: 1.4437e-04\n",
            "Epoch 23/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.9371e-04 - val_loss: 1.3371e-04\n",
            "Epoch 24/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.9626e-04 - val_loss: 1.3110e-04\n",
            "Epoch 25/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 3.0129e-04 - val_loss: 1.3517e-04\n",
            "Epoch 26/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.9249e-04 - val_loss: 1.1879e-04\n",
            "Epoch 27/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.9825e-04 - val_loss: 1.2572e-04\n",
            "Epoch 28/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.9908e-04 - val_loss: 1.3568e-04\n",
            "Epoch 29/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8601e-04 - val_loss: 1.1469e-04\n",
            "Epoch 30/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.8662e-04 - val_loss: 1.2472e-04\n",
            "Epoch 31/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.9077e-04 - val_loss: 1.2021e-04\n",
            "Epoch 32/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8357e-04 - val_loss: 1.2554e-04\n",
            "Epoch 33/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8737e-04 - val_loss: 1.3306e-04\n",
            "Epoch 34/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8649e-04 - val_loss: 1.5186e-04\n",
            "Epoch 35/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.9426e-04 - val_loss: 1.1411e-04\n",
            "Epoch 36/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8699e-04 - val_loss: 1.3304e-04\n",
            "Epoch 37/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8235e-04 - val_loss: 1.1517e-04\n",
            "Epoch 38/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8523e-04 - val_loss: 1.1078e-04\n",
            "Epoch 39/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8009e-04 - val_loss: 1.2015e-04\n",
            "Epoch 40/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.8383e-04 - val_loss: 1.2232e-04\n",
            "Epoch 41/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7757e-04 - val_loss: 1.1482e-04\n",
            "Epoch 42/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.8382e-04 - val_loss: 1.1441e-04\n",
            "Epoch 43/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.7701e-04 - val_loss: 1.3549e-04\n",
            "Epoch 44/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7413e-04 - val_loss: 1.1258e-04\n",
            "Epoch 45/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7470e-04 - val_loss: 1.1219e-04\n",
            "Epoch 46/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.7786e-04 - val_loss: 1.0512e-04\n",
            "Epoch 47/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7752e-04 - val_loss: 1.3833e-04\n",
            "Epoch 48/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7847e-04 - val_loss: 1.3893e-04\n",
            "Epoch 49/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.7233e-04 - val_loss: 1.1596e-04\n",
            "Epoch 50/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.7846e-04 - val_loss: 1.0774e-04\n",
            "Epoch 51/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7554e-04 - val_loss: 1.2281e-04\n",
            "Epoch 52/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.7206e-04 - val_loss: 1.0916e-04\n",
            "Epoch 53/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.6785e-04 - val_loss: 1.0966e-04\n",
            "Epoch 54/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6538e-04 - val_loss: 9.7000e-05\n",
            "Epoch 55/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.7158e-04 - val_loss: 1.2649e-04\n",
            "Epoch 56/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.7039e-04 - val_loss: 1.0757e-04\n",
            "Epoch 57/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.6463e-04 - val_loss: 1.1652e-04\n",
            "Epoch 58/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.6674e-04 - val_loss: 9.5819e-05\n",
            "Epoch 59/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6535e-04 - val_loss: 1.1539e-04\n",
            "Epoch 60/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6887e-04 - val_loss: 1.0048e-04\n",
            "Epoch 61/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6471e-04 - val_loss: 1.0678e-04\n",
            "Epoch 62/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6055e-04 - val_loss: 9.7088e-05\n",
            "Epoch 63/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5841e-04 - val_loss: 1.0237e-04\n",
            "Epoch 64/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5846e-04 - val_loss: 1.1042e-04\n",
            "Epoch 65/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5862e-04 - val_loss: 9.8929e-05\n",
            "Epoch 66/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.6557e-04 - val_loss: 1.0156e-04\n",
            "Epoch 67/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5876e-04 - val_loss: 9.0748e-05\n",
            "Epoch 68/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5537e-04 - val_loss: 1.0251e-04\n",
            "Epoch 69/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.6111e-04 - val_loss: 1.0664e-04\n",
            "Epoch 70/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6444e-04 - val_loss: 1.0178e-04\n",
            "Epoch 71/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5560e-04 - val_loss: 1.0857e-04\n",
            "Epoch 72/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5870e-04 - val_loss: 1.0762e-04\n",
            "Epoch 73/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5635e-04 - val_loss: 1.1555e-04\n",
            "Epoch 74/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5981e-04 - val_loss: 1.0512e-04\n",
            "Epoch 75/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5876e-04 - val_loss: 9.7287e-05\n",
            "Epoch 76/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5706e-04 - val_loss: 1.1666e-04\n",
            "Epoch 77/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5140e-04 - val_loss: 9.4270e-05\n",
            "Epoch 78/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5565e-04 - val_loss: 9.3881e-05\n",
            "Epoch 79/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5844e-04 - val_loss: 9.3704e-05\n",
            "Epoch 80/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5586e-04 - val_loss: 1.1164e-04\n",
            "Epoch 81/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5392e-04 - val_loss: 8.6073e-05\n",
            "Epoch 82/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5321e-04 - val_loss: 9.2984e-05\n",
            "Epoch 83/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5348e-04 - val_loss: 1.0184e-04\n",
            "Epoch 84/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5241e-04 - val_loss: 8.5268e-05\n",
            "Epoch 85/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5761e-04 - val_loss: 9.2984e-05\n",
            "Epoch 86/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4450e-04 - val_loss: 9.1945e-05\n",
            "Epoch 87/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4990e-04 - val_loss: 9.1458e-05\n",
            "Epoch 88/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5363e-04 - val_loss: 9.2495e-05\n",
            "Epoch 89/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.6098e-04 - val_loss: 9.3557e-05\n",
            "Epoch 90/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5552e-04 - val_loss: 1.0884e-04\n",
            "Epoch 91/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4954e-04 - val_loss: 8.9459e-05\n",
            "Epoch 92/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4867e-04 - val_loss: 9.4108e-05\n",
            "Epoch 93/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5349e-04 - val_loss: 8.8721e-05\n",
            "Epoch 94/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5509e-04 - val_loss: 9.5435e-05\n",
            "Epoch 95/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5458e-04 - val_loss: 9.0850e-05\n",
            "Epoch 96/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5137e-04 - val_loss: 8.7336e-05\n",
            "Epoch 97/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5210e-04 - val_loss: 8.8037e-05\n",
            "Epoch 98/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5069e-04 - val_loss: 8.2888e-05\n",
            "Epoch 99/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5556e-04 - val_loss: 9.0266e-05\n",
            "Epoch 100/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4697e-04 - val_loss: 8.9192e-05\n",
            "Epoch 101/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4955e-04 - val_loss: 9.0513e-05\n",
            "Epoch 102/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5269e-04 - val_loss: 9.0004e-05\n",
            "Epoch 103/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4694e-04 - val_loss: 9.3421e-05\n",
            "Epoch 104/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5155e-04 - val_loss: 9.5527e-05\n",
            "Epoch 105/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4869e-04 - val_loss: 1.0042e-04\n",
            "Epoch 106/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5424e-04 - val_loss: 8.9210e-05\n",
            "Epoch 107/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4959e-04 - val_loss: 9.2900e-05\n",
            "Epoch 108/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4791e-04 - val_loss: 8.1794e-05\n",
            "Epoch 109/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.5563e-04 - val_loss: 8.4071e-05\n",
            "Epoch 110/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4549e-04 - val_loss: 9.3193e-05\n",
            "Epoch 111/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4868e-04 - val_loss: 9.1486e-05\n",
            "Epoch 112/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4623e-04 - val_loss: 8.2940e-05\n",
            "Epoch 113/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4950e-04 - val_loss: 8.9365e-05\n",
            "Epoch 114/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5158e-04 - val_loss: 8.9994e-05\n",
            "Epoch 115/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4729e-04 - val_loss: 9.3657e-05\n",
            "Epoch 116/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4927e-04 - val_loss: 8.6861e-05\n",
            "Epoch 117/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4361e-04 - val_loss: 9.1193e-05\n",
            "Epoch 118/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4999e-04 - val_loss: 9.3935e-05\n",
            "Epoch 119/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4662e-04 - val_loss: 9.7839e-05\n",
            "Epoch 120/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4991e-04 - val_loss: 8.1040e-05\n",
            "Epoch 121/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4152e-04 - val_loss: 8.3610e-05\n",
            "Epoch 122/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4494e-04 - val_loss: 8.9035e-05\n",
            "Epoch 123/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4478e-04 - val_loss: 8.5496e-05\n",
            "Epoch 124/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4334e-04 - val_loss: 7.6048e-05\n",
            "Epoch 125/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4307e-04 - val_loss: 8.5771e-05\n",
            "Epoch 126/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4932e-04 - val_loss: 9.2720e-05\n",
            "Epoch 127/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4205e-04 - val_loss: 9.5036e-05\n",
            "Epoch 128/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4259e-04 - val_loss: 7.9964e-05\n",
            "Epoch 129/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4113e-04 - val_loss: 8.9014e-05\n",
            "Epoch 130/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.5263e-04 - val_loss: 9.3659e-05\n",
            "Epoch 131/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4413e-04 - val_loss: 8.1486e-05\n",
            "Epoch 132/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4551e-04 - val_loss: 8.3571e-05\n",
            "Epoch 133/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4891e-04 - val_loss: 9.2126e-05\n",
            "Epoch 134/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4837e-04 - val_loss: 8.8169e-05\n",
            "Epoch 135/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4456e-04 - val_loss: 7.9578e-05\n",
            "Epoch 136/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4796e-04 - val_loss: 9.7430e-05\n",
            "Epoch 137/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4503e-04 - val_loss: 8.9983e-05\n",
            "Epoch 138/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3774e-04 - val_loss: 8.3673e-05\n",
            "Epoch 139/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4726e-04 - val_loss: 8.4720e-05\n",
            "Epoch 140/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4815e-04 - val_loss: 8.2522e-05\n",
            "Epoch 141/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4139e-04 - val_loss: 8.3882e-05\n",
            "Epoch 142/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4528e-04 - val_loss: 8.7435e-05\n",
            "Epoch 143/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4544e-04 - val_loss: 8.7889e-05\n",
            "Epoch 144/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4551e-04 - val_loss: 8.0125e-05\n",
            "Epoch 145/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4604e-04 - val_loss: 9.0015e-05\n",
            "Epoch 146/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4492e-04 - val_loss: 8.2246e-05\n",
            "Epoch 147/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4237e-04 - val_loss: 7.6904e-05\n",
            "Epoch 148/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3818e-04 - val_loss: 8.5210e-05\n",
            "Epoch 149/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4549e-04 - val_loss: 8.1724e-05\n",
            "Epoch 150/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3947e-04 - val_loss: 7.7630e-05\n",
            "Epoch 151/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4566e-04 - val_loss: 8.1007e-05\n",
            "Epoch 152/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4515e-04 - val_loss: 7.5418e-05\n",
            "Epoch 153/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4246e-04 - val_loss: 7.8684e-05\n",
            "Epoch 154/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4245e-04 - val_loss: 7.8605e-05\n",
            "Epoch 155/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3843e-04 - val_loss: 8.1726e-05\n",
            "Epoch 156/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4381e-04 - val_loss: 9.4967e-05\n",
            "Epoch 157/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4487e-04 - val_loss: 7.9357e-05\n",
            "Epoch 158/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4497e-04 - val_loss: 8.1035e-05\n",
            "Epoch 159/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4029e-04 - val_loss: 9.6592e-05\n",
            "Epoch 160/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3917e-04 - val_loss: 9.5115e-05\n",
            "Epoch 161/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4035e-04 - val_loss: 8.1750e-05\n",
            "Epoch 162/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4079e-04 - val_loss: 8.5101e-05\n",
            "Epoch 163/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3720e-04 - val_loss: 7.6889e-05\n",
            "Epoch 164/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4364e-04 - val_loss: 7.7653e-05\n",
            "Epoch 165/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4264e-04 - val_loss: 8.0213e-05\n",
            "Epoch 166/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3782e-04 - val_loss: 8.9934e-05\n",
            "Epoch 167/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3703e-04 - val_loss: 9.5250e-05\n",
            "Epoch 168/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4031e-04 - val_loss: 8.6950e-05\n",
            "Epoch 169/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4951e-04 - val_loss: 8.1192e-05\n",
            "Epoch 170/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3980e-04 - val_loss: 8.5895e-05\n",
            "Epoch 171/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4098e-04 - val_loss: 8.1608e-05\n",
            "Epoch 172/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4294e-04 - val_loss: 8.2714e-05\n",
            "Epoch 173/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4282e-04 - val_loss: 7.9014e-05\n",
            "Epoch 174/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3989e-04 - val_loss: 7.7924e-05\n",
            "Epoch 175/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3502e-04 - val_loss: 8.3630e-05\n",
            "Epoch 176/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3452e-04 - val_loss: 7.9896e-05\n",
            "Epoch 177/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3713e-04 - val_loss: 8.0761e-05\n",
            "Epoch 178/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4138e-04 - val_loss: 8.4083e-05\n",
            "Epoch 179/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3717e-04 - val_loss: 7.5097e-05\n",
            "Epoch 180/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4275e-04 - val_loss: 7.8155e-05\n",
            "Epoch 181/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3831e-04 - val_loss: 8.1488e-05\n",
            "Epoch 182/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3721e-04 - val_loss: 8.7553e-05\n",
            "Epoch 183/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3721e-04 - val_loss: 8.0106e-05\n",
            "Epoch 184/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3568e-04 - val_loss: 8.8454e-05\n",
            "Epoch 185/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4295e-04 - val_loss: 8.4685e-05\n",
            "Epoch 186/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3579e-04 - val_loss: 7.7798e-05\n",
            "Epoch 187/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3532e-04 - val_loss: 7.8290e-05\n",
            "Epoch 188/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3690e-04 - val_loss: 7.9805e-05\n",
            "Epoch 189/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3672e-04 - val_loss: 9.7818e-05\n",
            "Epoch 190/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3870e-04 - val_loss: 8.7351e-05\n",
            "Epoch 191/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4260e-04 - val_loss: 8.0000e-05\n",
            "Epoch 192/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3671e-04 - val_loss: 8.9733e-05\n",
            "Epoch 193/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3318e-04 - val_loss: 8.6893e-05\n",
            "Epoch 194/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4006e-04 - val_loss: 8.9684e-05\n",
            "Epoch 195/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3837e-04 - val_loss: 8.3079e-05\n",
            "Epoch 196/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3772e-04 - val_loss: 8.0423e-05\n",
            "Epoch 197/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3176e-04 - val_loss: 9.0000e-05\n",
            "Epoch 198/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3518e-04 - val_loss: 7.8977e-05\n",
            "Epoch 199/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3594e-04 - val_loss: 9.4545e-05\n",
            "Epoch 200/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3849e-04 - val_loss: 7.8785e-05\n",
            "Epoch 201/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3685e-04 - val_loss: 7.9576e-05\n",
            "Epoch 202/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3421e-04 - val_loss: 7.6307e-05\n",
            "Epoch 203/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3753e-04 - val_loss: 8.3350e-05\n",
            "Epoch 204/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3743e-04 - val_loss: 9.5805e-05\n",
            "Epoch 205/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3916e-04 - val_loss: 8.2744e-05\n",
            "Epoch 206/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3785e-04 - val_loss: 8.6072e-05\n",
            "Epoch 207/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4236e-04 - val_loss: 8.6318e-05\n",
            "Epoch 208/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3328e-04 - val_loss: 7.9259e-05\n",
            "Epoch 209/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3287e-04 - val_loss: 8.0858e-05\n",
            "Epoch 210/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3381e-04 - val_loss: 8.3605e-05\n",
            "Epoch 211/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3240e-04 - val_loss: 8.2493e-05\n",
            "Epoch 212/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3392e-04 - val_loss: 7.5129e-05\n",
            "Epoch 213/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3834e-04 - val_loss: 7.2581e-05\n",
            "Epoch 214/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4156e-04 - val_loss: 8.0345e-05\n",
            "Epoch 215/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3767e-04 - val_loss: 7.9197e-05\n",
            "Epoch 216/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3356e-04 - val_loss: 8.3966e-05\n",
            "Epoch 217/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3509e-04 - val_loss: 7.8705e-05\n",
            "Epoch 218/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3226e-04 - val_loss: 8.2005e-05\n",
            "Epoch 219/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3707e-04 - val_loss: 7.6945e-05\n",
            "Epoch 220/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3669e-04 - val_loss: 7.9184e-05\n",
            "Epoch 221/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3646e-04 - val_loss: 8.1153e-05\n",
            "Epoch 222/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.4074e-04 - val_loss: 8.1318e-05\n",
            "Epoch 223/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3917e-04 - val_loss: 8.0942e-05\n",
            "Epoch 224/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4043e-04 - val_loss: 9.1994e-05\n",
            "Epoch 225/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3043e-04 - val_loss: 8.3130e-05\n",
            "Epoch 226/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4015e-04 - val_loss: 8.0402e-05\n",
            "Epoch 227/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3320e-04 - val_loss: 7.7480e-05\n",
            "Epoch 228/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3276e-04 - val_loss: 7.7076e-05\n",
            "Epoch 229/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3920e-04 - val_loss: 7.9377e-05\n",
            "Epoch 230/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3475e-04 - val_loss: 7.8556e-05\n",
            "Epoch 231/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3847e-04 - val_loss: 8.1389e-05\n",
            "Epoch 232/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3140e-04 - val_loss: 7.4542e-05\n",
            "Epoch 233/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3813e-04 - val_loss: 7.9244e-05\n",
            "Epoch 234/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3635e-04 - val_loss: 7.7221e-05\n",
            "Epoch 235/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3683e-04 - val_loss: 7.4852e-05\n",
            "Epoch 236/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3025e-04 - val_loss: 7.5229e-05\n",
            "Epoch 237/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3096e-04 - val_loss: 7.6821e-05\n",
            "Epoch 238/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3027e-04 - val_loss: 8.1792e-05\n",
            "Epoch 239/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3173e-04 - val_loss: 8.0432e-05\n",
            "Epoch 240/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3280e-04 - val_loss: 7.8267e-05\n",
            "Epoch 241/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3640e-04 - val_loss: 7.9056e-05\n",
            "Epoch 242/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3853e-04 - val_loss: 8.7576e-05\n",
            "Epoch 243/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3237e-04 - val_loss: 7.7446e-05\n",
            "Epoch 244/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3161e-04 - val_loss: 9.1113e-05\n",
            "Epoch 245/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3487e-04 - val_loss: 7.6241e-05\n",
            "Epoch 246/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3154e-04 - val_loss: 8.5191e-05\n",
            "Epoch 247/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3576e-04 - val_loss: 8.1330e-05\n",
            "Epoch 248/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3061e-04 - val_loss: 7.9192e-05\n",
            "Epoch 249/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3619e-04 - val_loss: 7.6940e-05\n",
            "Epoch 250/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3373e-04 - val_loss: 7.9062e-05\n",
            "Epoch 251/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.4109e-04 - val_loss: 7.5459e-05\n",
            "Epoch 252/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3592e-04 - val_loss: 7.4931e-05\n",
            "Epoch 253/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3669e-04 - val_loss: 7.2426e-05\n",
            "Epoch 254/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3495e-04 - val_loss: 8.5211e-05\n",
            "Epoch 255/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3361e-04 - val_loss: 7.6930e-05\n",
            "Epoch 256/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3021e-04 - val_loss: 7.9013e-05\n",
            "Epoch 257/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3428e-04 - val_loss: 7.6071e-05\n",
            "Epoch 258/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3481e-04 - val_loss: 7.6145e-05\n",
            "Epoch 259/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3057e-04 - val_loss: 7.7423e-05\n",
            "Epoch 260/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2665e-04 - val_loss: 8.0509e-05\n",
            "Epoch 261/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3251e-04 - val_loss: 8.3342e-05\n",
            "Epoch 262/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3527e-04 - val_loss: 8.8446e-05\n",
            "Epoch 263/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3051e-04 - val_loss: 8.0273e-05\n",
            "Epoch 264/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3374e-04 - val_loss: 7.1155e-05\n",
            "Epoch 265/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3090e-04 - val_loss: 7.9493e-05\n",
            "Epoch 266/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3805e-04 - val_loss: 7.4620e-05\n",
            "Epoch 267/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3403e-04 - val_loss: 7.8000e-05\n",
            "Epoch 268/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3196e-04 - val_loss: 7.2866e-05\n",
            "Epoch 269/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3163e-04 - val_loss: 8.0528e-05\n",
            "Epoch 270/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3222e-04 - val_loss: 6.8428e-05\n",
            "Epoch 271/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2865e-04 - val_loss: 7.3418e-05\n",
            "Epoch 272/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2943e-04 - val_loss: 7.6928e-05\n",
            "Epoch 273/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3413e-04 - val_loss: 7.2151e-05\n",
            "Epoch 274/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3750e-04 - val_loss: 8.2418e-05\n",
            "Epoch 275/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3008e-04 - val_loss: 8.3170e-05\n",
            "Epoch 276/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3191e-04 - val_loss: 8.7484e-05\n",
            "Epoch 277/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3027e-04 - val_loss: 8.5567e-05\n",
            "Epoch 278/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3073e-04 - val_loss: 7.3153e-05\n",
            "Epoch 279/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2499e-04 - val_loss: 7.0253e-05\n",
            "Epoch 280/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3504e-04 - val_loss: 7.0897e-05\n",
            "Epoch 281/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3078e-04 - val_loss: 7.9560e-05\n",
            "Epoch 282/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3014e-04 - val_loss: 7.2102e-05\n",
            "Epoch 283/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3125e-04 - val_loss: 9.0373e-05\n",
            "Epoch 284/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3028e-04 - val_loss: 7.6787e-05\n",
            "Epoch 285/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3457e-04 - val_loss: 7.2537e-05\n",
            "Epoch 286/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3265e-04 - val_loss: 7.3732e-05\n",
            "Epoch 287/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2746e-04 - val_loss: 7.9707e-05\n",
            "Epoch 288/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3501e-04 - val_loss: 7.6528e-05\n",
            "Epoch 289/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3091e-04 - val_loss: 7.0529e-05\n",
            "Epoch 290/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3277e-04 - val_loss: 7.4599e-05\n",
            "Epoch 291/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3099e-04 - val_loss: 7.8179e-05\n",
            "Epoch 292/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3140e-04 - val_loss: 9.1721e-05\n",
            "Epoch 293/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2824e-04 - val_loss: 8.6503e-05\n",
            "Epoch 294/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3057e-04 - val_loss: 7.4991e-05\n",
            "Epoch 295/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3232e-04 - val_loss: 6.9068e-05\n",
            "Epoch 296/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3421e-04 - val_loss: 7.9433e-05\n",
            "Epoch 297/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2744e-04 - val_loss: 7.6854e-05\n",
            "Epoch 298/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2985e-04 - val_loss: 7.2977e-05\n",
            "Epoch 299/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2895e-04 - val_loss: 7.7121e-05\n",
            "Epoch 300/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3453e-04 - val_loss: 7.7962e-05\n",
            "Epoch 301/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3152e-04 - val_loss: 7.7051e-05\n",
            "Epoch 302/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2858e-04 - val_loss: 7.3763e-05\n",
            "Epoch 303/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2890e-04 - val_loss: 7.2174e-05\n",
            "Epoch 304/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2568e-04 - val_loss: 7.6356e-05\n",
            "Epoch 305/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2833e-04 - val_loss: 7.6030e-05\n",
            "Epoch 306/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2620e-04 - val_loss: 7.7456e-05\n",
            "Epoch 307/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2530e-04 - val_loss: 7.6727e-05\n",
            "Epoch 308/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2974e-04 - val_loss: 8.2424e-05\n",
            "Epoch 309/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3156e-04 - val_loss: 7.8232e-05\n",
            "Epoch 310/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2709e-04 - val_loss: 7.8720e-05\n",
            "Epoch 311/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3138e-04 - val_loss: 8.3510e-05\n",
            "Epoch 312/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3328e-04 - val_loss: 8.0072e-05\n",
            "Epoch 313/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2886e-04 - val_loss: 7.2390e-05\n",
            "Epoch 314/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3085e-04 - val_loss: 7.5694e-05\n",
            "Epoch 315/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2862e-04 - val_loss: 7.4803e-05\n",
            "Epoch 316/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3071e-04 - val_loss: 7.5888e-05\n",
            "Epoch 317/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3178e-04 - val_loss: 8.0052e-05\n",
            "Epoch 318/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3354e-04 - val_loss: 7.9626e-05\n",
            "Epoch 319/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2985e-04 - val_loss: 7.3814e-05\n",
            "Epoch 320/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2619e-04 - val_loss: 7.5024e-05\n",
            "Epoch 321/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3392e-04 - val_loss: 6.7439e-05\n",
            "Epoch 322/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2986e-04 - val_loss: 8.5076e-05\n",
            "Epoch 323/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3026e-04 - val_loss: 7.5195e-05\n",
            "Epoch 324/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2823e-04 - val_loss: 7.5631e-05\n",
            "Epoch 325/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3123e-04 - val_loss: 7.9174e-05\n",
            "Epoch 326/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2933e-04 - val_loss: 7.1896e-05\n",
            "Epoch 327/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3026e-04 - val_loss: 7.1944e-05\n",
            "Epoch 328/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3256e-04 - val_loss: 7.0955e-05\n",
            "Epoch 329/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3236e-04 - val_loss: 8.7013e-05\n",
            "Epoch 330/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2908e-04 - val_loss: 7.3462e-05\n",
            "Epoch 331/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2563e-04 - val_loss: 7.6412e-05\n",
            "Epoch 332/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2515e-04 - val_loss: 7.0234e-05\n",
            "Epoch 333/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3182e-04 - val_loss: 7.8474e-05\n",
            "Epoch 334/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2914e-04 - val_loss: 7.7991e-05\n",
            "Epoch 335/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2697e-04 - val_loss: 7.4538e-05\n",
            "Epoch 336/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2793e-04 - val_loss: 7.2411e-05\n",
            "Epoch 337/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2750e-04 - val_loss: 7.4134e-05\n",
            "Epoch 338/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2958e-04 - val_loss: 7.5754e-05\n",
            "Epoch 339/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2514e-04 - val_loss: 8.1008e-05\n",
            "Epoch 340/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3021e-04 - val_loss: 6.7805e-05\n",
            "Epoch 341/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2396e-04 - val_loss: 7.6422e-05\n",
            "Epoch 342/1000\n",
            "240/240 [==============================] - 2s 10ms/step - loss: 2.2759e-04 - val_loss: 7.5303e-05\n",
            "Epoch 343/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2355e-04 - val_loss: 7.8723e-05\n",
            "Epoch 344/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2641e-04 - val_loss: 7.5278e-05\n",
            "Epoch 345/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2715e-04 - val_loss: 7.4297e-05\n",
            "Epoch 346/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2533e-04 - val_loss: 7.5399e-05\n",
            "Epoch 347/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3100e-04 - val_loss: 7.8251e-05\n",
            "Epoch 348/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2928e-04 - val_loss: 6.9452e-05\n",
            "Epoch 349/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2761e-04 - val_loss: 7.3425e-05\n",
            "Epoch 350/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2707e-04 - val_loss: 7.1514e-05\n",
            "Epoch 351/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2983e-04 - val_loss: 7.1068e-05\n",
            "Epoch 352/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3370e-04 - val_loss: 7.0979e-05\n",
            "Epoch 353/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2986e-04 - val_loss: 7.2591e-05\n",
            "Epoch 354/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3023e-04 - val_loss: 7.2381e-05\n",
            "Epoch 355/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2872e-04 - val_loss: 8.7415e-05\n",
            "Epoch 356/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2791e-04 - val_loss: 7.7782e-05\n",
            "Epoch 357/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.3035e-04 - val_loss: 8.5147e-05\n",
            "Epoch 358/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2840e-04 - val_loss: 7.7275e-05\n",
            "Epoch 359/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2747e-04 - val_loss: 7.7018e-05\n",
            "Epoch 360/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2744e-04 - val_loss: 7.9255e-05\n",
            "Epoch 361/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2835e-04 - val_loss: 7.4366e-05\n",
            "Epoch 362/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2326e-04 - val_loss: 7.2367e-05\n",
            "Epoch 363/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2571e-04 - val_loss: 6.9577e-05\n",
            "Epoch 364/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2463e-04 - val_loss: 7.3788e-05\n",
            "Epoch 365/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2856e-04 - val_loss: 6.9145e-05\n",
            "Epoch 366/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2570e-04 - val_loss: 7.5419e-05\n",
            "Epoch 367/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2989e-04 - val_loss: 7.6255e-05\n",
            "Epoch 368/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3013e-04 - val_loss: 8.4330e-05\n",
            "Epoch 369/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2406e-04 - val_loss: 8.4918e-05\n",
            "Epoch 370/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2560e-04 - val_loss: 7.0746e-05\n",
            "Epoch 371/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2779e-04 - val_loss: 7.3333e-05\n",
            "Epoch 372/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2890e-04 - val_loss: 8.1250e-05\n",
            "Epoch 373/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2953e-04 - val_loss: 7.6519e-05\n",
            "Epoch 374/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2534e-04 - val_loss: 7.4681e-05\n",
            "Epoch 375/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3365e-04 - val_loss: 6.8280e-05\n",
            "Epoch 376/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2559e-04 - val_loss: 7.7393e-05\n",
            "Epoch 377/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2668e-04 - val_loss: 7.6131e-05\n",
            "Epoch 378/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2777e-04 - val_loss: 7.4832e-05\n",
            "Epoch 379/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2143e-04 - val_loss: 7.1157e-05\n",
            "Epoch 380/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2622e-04 - val_loss: 7.5368e-05\n",
            "Epoch 381/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2540e-04 - val_loss: 7.3686e-05\n",
            "Epoch 382/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3306e-04 - val_loss: 8.1631e-05\n",
            "Epoch 383/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3063e-04 - val_loss: 8.3511e-05\n",
            "Epoch 384/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3012e-04 - val_loss: 7.4400e-05\n",
            "Epoch 385/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2951e-04 - val_loss: 7.9301e-05\n",
            "Epoch 386/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2480e-04 - val_loss: 7.6226e-05\n",
            "Epoch 387/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2947e-04 - val_loss: 7.4912e-05\n",
            "Epoch 388/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2354e-04 - val_loss: 7.3186e-05\n",
            "Epoch 389/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3011e-04 - val_loss: 7.3063e-05\n",
            "Epoch 390/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2560e-04 - val_loss: 7.0112e-05\n",
            "Epoch 391/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2606e-04 - val_loss: 8.3726e-05\n",
            "Epoch 392/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2655e-04 - val_loss: 7.5330e-05\n",
            "Epoch 393/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2550e-04 - val_loss: 7.7057e-05\n",
            "Epoch 394/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2729e-04 - val_loss: 7.0612e-05\n",
            "Epoch 395/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2919e-04 - val_loss: 7.3127e-05\n",
            "Epoch 396/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2782e-04 - val_loss: 8.4668e-05\n",
            "Epoch 397/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2423e-04 - val_loss: 6.9867e-05\n",
            "Epoch 398/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2414e-04 - val_loss: 6.8555e-05\n",
            "Epoch 399/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3034e-04 - val_loss: 7.0032e-05\n",
            "Epoch 400/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2516e-04 - val_loss: 8.1051e-05\n",
            "Epoch 401/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2413e-04 - val_loss: 7.4799e-05\n",
            "Epoch 402/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3011e-04 - val_loss: 7.3880e-05\n",
            "Epoch 403/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2846e-04 - val_loss: 6.6040e-05\n",
            "Epoch 404/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2079e-04 - val_loss: 7.1750e-05\n",
            "Epoch 405/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2102e-04 - val_loss: 7.3703e-05\n",
            "Epoch 406/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2732e-04 - val_loss: 6.8825e-05\n",
            "Epoch 407/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2366e-04 - val_loss: 7.1211e-05\n",
            "Epoch 408/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2650e-04 - val_loss: 7.2143e-05\n",
            "Epoch 409/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2773e-04 - val_loss: 6.8814e-05\n",
            "Epoch 410/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2894e-04 - val_loss: 6.9651e-05\n",
            "Epoch 411/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2842e-04 - val_loss: 8.6229e-05\n",
            "Epoch 412/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3002e-04 - val_loss: 7.7930e-05\n",
            "Epoch 413/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2420e-04 - val_loss: 7.7114e-05\n",
            "Epoch 414/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2448e-04 - val_loss: 7.5256e-05\n",
            "Epoch 415/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2476e-04 - val_loss: 7.2095e-05\n",
            "Epoch 416/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2426e-04 - val_loss: 7.3384e-05\n",
            "Epoch 417/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2715e-04 - val_loss: 7.6404e-05\n",
            "Epoch 418/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3064e-04 - val_loss: 6.8916e-05\n",
            "Epoch 419/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2379e-04 - val_loss: 7.4648e-05\n",
            "Epoch 420/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2132e-04 - val_loss: 7.6196e-05\n",
            "Epoch 421/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2709e-04 - val_loss: 7.4560e-05\n",
            "Epoch 422/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2189e-04 - val_loss: 6.9585e-05\n",
            "Epoch 423/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2499e-04 - val_loss: 7.4237e-05\n",
            "Epoch 424/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2313e-04 - val_loss: 7.5669e-05\n",
            "Epoch 425/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2248e-04 - val_loss: 7.4003e-05\n",
            "Epoch 426/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3310e-04 - val_loss: 6.8714e-05\n",
            "Epoch 427/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2701e-04 - val_loss: 7.4602e-05\n",
            "Epoch 428/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2756e-04 - val_loss: 7.5830e-05\n",
            "Epoch 429/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2259e-04 - val_loss: 7.1633e-05\n",
            "Epoch 430/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2476e-04 - val_loss: 7.5511e-05\n",
            "Epoch 431/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2655e-04 - val_loss: 7.5200e-05\n",
            "Epoch 432/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2807e-04 - val_loss: 7.4969e-05\n",
            "Epoch 433/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2404e-04 - val_loss: 7.4701e-05\n",
            "Epoch 434/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2715e-04 - val_loss: 8.0274e-05\n",
            "Epoch 435/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2305e-04 - val_loss: 7.6340e-05\n",
            "Epoch 436/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2661e-04 - val_loss: 7.0040e-05\n",
            "Epoch 437/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2771e-04 - val_loss: 7.4472e-05\n",
            "Epoch 438/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2868e-04 - val_loss: 7.8183e-05\n",
            "Epoch 439/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2587e-04 - val_loss: 6.5846e-05\n",
            "Epoch 440/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2172e-04 - val_loss: 7.2618e-05\n",
            "Epoch 441/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2567e-04 - val_loss: 7.1302e-05\n",
            "Epoch 442/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2030e-04 - val_loss: 6.9238e-05\n",
            "Epoch 443/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2405e-04 - val_loss: 7.1066e-05\n",
            "Epoch 444/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2576e-04 - val_loss: 6.9604e-05\n",
            "Epoch 445/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2327e-04 - val_loss: 7.4864e-05\n",
            "Epoch 446/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2826e-04 - val_loss: 7.4092e-05\n",
            "Epoch 447/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2716e-04 - val_loss: 6.9978e-05\n",
            "Epoch 448/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2491e-04 - val_loss: 7.1713e-05\n",
            "Epoch 449/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2643e-04 - val_loss: 7.6461e-05\n",
            "Epoch 450/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2358e-04 - val_loss: 7.4873e-05\n",
            "Epoch 451/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2920e-04 - val_loss: 7.3480e-05\n",
            "Epoch 452/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2476e-04 - val_loss: 7.9546e-05\n",
            "Epoch 453/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2881e-04 - val_loss: 7.3945e-05\n",
            "Epoch 454/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2594e-04 - val_loss: 7.7304e-05\n",
            "Epoch 455/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2027e-04 - val_loss: 7.0866e-05\n",
            "Epoch 456/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2523e-04 - val_loss: 6.7430e-05\n",
            "Epoch 457/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3054e-04 - val_loss: 7.7650e-05\n",
            "Epoch 458/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2767e-04 - val_loss: 7.3427e-05\n",
            "Epoch 459/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2037e-04 - val_loss: 7.3420e-05\n",
            "Epoch 460/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2161e-04 - val_loss: 7.0401e-05\n",
            "Epoch 461/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2468e-04 - val_loss: 7.4140e-05\n",
            "Epoch 462/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2579e-04 - val_loss: 7.0100e-05\n",
            "Epoch 463/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2685e-04 - val_loss: 6.9071e-05\n",
            "Epoch 464/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2864e-04 - val_loss: 6.8739e-05\n",
            "Epoch 465/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2787e-04 - val_loss: 7.3767e-05\n",
            "Epoch 466/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2419e-04 - val_loss: 6.6878e-05\n",
            "Epoch 467/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2873e-04 - val_loss: 7.6718e-05\n",
            "Epoch 468/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2609e-04 - val_loss: 7.1446e-05\n",
            "Epoch 469/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2403e-04 - val_loss: 7.8880e-05\n",
            "Epoch 470/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2310e-04 - val_loss: 6.6396e-05\n",
            "Epoch 471/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2143e-04 - val_loss: 6.9493e-05\n",
            "Epoch 472/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2382e-04 - val_loss: 7.0314e-05\n",
            "Epoch 473/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1884e-04 - val_loss: 6.8470e-05\n",
            "Epoch 474/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2655e-04 - val_loss: 8.2246e-05\n",
            "Epoch 475/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2428e-04 - val_loss: 7.6353e-05\n",
            "Epoch 476/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2407e-04 - val_loss: 7.3686e-05\n",
            "Epoch 477/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2881e-04 - val_loss: 7.6777e-05\n",
            "Epoch 478/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2447e-04 - val_loss: 7.6714e-05\n",
            "Epoch 479/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2170e-04 - val_loss: 7.1421e-05\n",
            "Epoch 480/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2424e-04 - val_loss: 7.8524e-05\n",
            "Epoch 481/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2240e-04 - val_loss: 6.8533e-05\n",
            "Epoch 482/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2079e-04 - val_loss: 7.4632e-05\n",
            "Epoch 483/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2270e-04 - val_loss: 7.4493e-05\n",
            "Epoch 484/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2165e-04 - val_loss: 7.2545e-05\n",
            "Epoch 485/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2645e-04 - val_loss: 7.6854e-05\n",
            "Epoch 486/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2445e-04 - val_loss: 7.3972e-05\n",
            "Epoch 487/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2640e-04 - val_loss: 7.5180e-05\n",
            "Epoch 488/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2829e-04 - val_loss: 7.2364e-05\n",
            "Epoch 489/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2116e-04 - val_loss: 7.0968e-05\n",
            "Epoch 490/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2332e-04 - val_loss: 7.0241e-05\n",
            "Epoch 491/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2550e-04 - val_loss: 6.9688e-05\n",
            "Epoch 492/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2456e-04 - val_loss: 7.4510e-05\n",
            "Epoch 493/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2593e-04 - val_loss: 7.0193e-05\n",
            "Epoch 494/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2052e-04 - val_loss: 7.4498e-05\n",
            "Epoch 495/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2696e-04 - val_loss: 7.1559e-05\n",
            "Epoch 496/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2378e-04 - val_loss: 7.5192e-05\n",
            "Epoch 497/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2155e-04 - val_loss: 7.4284e-05\n",
            "Epoch 498/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2485e-04 - val_loss: 6.9287e-05\n",
            "Epoch 499/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2456e-04 - val_loss: 7.0746e-05\n",
            "Epoch 500/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2369e-04 - val_loss: 7.0679e-05\n",
            "Epoch 501/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2147e-04 - val_loss: 7.1426e-05\n",
            "Epoch 502/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2374e-04 - val_loss: 7.5155e-05\n",
            "Epoch 503/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2196e-04 - val_loss: 7.0355e-05\n",
            "Epoch 504/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2548e-04 - val_loss: 6.8399e-05\n",
            "Epoch 505/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2452e-04 - val_loss: 6.6981e-05\n",
            "Epoch 506/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2553e-04 - val_loss: 6.7446e-05\n",
            "Epoch 507/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2397e-04 - val_loss: 7.2452e-05\n",
            "Epoch 508/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1961e-04 - val_loss: 8.2141e-05\n",
            "Epoch 509/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2407e-04 - val_loss: 6.7757e-05\n",
            "Epoch 510/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2261e-04 - val_loss: 6.8370e-05\n",
            "Epoch 511/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2357e-04 - val_loss: 6.6421e-05\n",
            "Epoch 512/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2145e-04 - val_loss: 7.0887e-05\n",
            "Epoch 513/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2401e-04 - val_loss: 7.8390e-05\n",
            "Epoch 514/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2251e-04 - val_loss: 6.7871e-05\n",
            "Epoch 515/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2382e-04 - val_loss: 7.5165e-05\n",
            "Epoch 516/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1895e-04 - val_loss: 7.1728e-05\n",
            "Epoch 517/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2290e-04 - val_loss: 6.9601e-05\n",
            "Epoch 518/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2025e-04 - val_loss: 6.6287e-05\n",
            "Epoch 519/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2391e-04 - val_loss: 6.8163e-05\n",
            "Epoch 520/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2275e-04 - val_loss: 7.4101e-05\n",
            "Epoch 521/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.3075e-04 - val_loss: 7.3771e-05\n",
            "Epoch 522/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2237e-04 - val_loss: 7.0358e-05\n",
            "Epoch 523/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1686e-04 - val_loss: 7.8275e-05\n",
            "Epoch 524/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2019e-04 - val_loss: 7.6881e-05\n",
            "Epoch 525/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2221e-04 - val_loss: 7.1684e-05\n",
            "Epoch 526/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2373e-04 - val_loss: 7.2382e-05\n",
            "Epoch 527/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2111e-04 - val_loss: 6.6564e-05\n",
            "Epoch 528/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2338e-04 - val_loss: 7.8070e-05\n",
            "Epoch 529/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2237e-04 - val_loss: 7.4660e-05\n",
            "Epoch 530/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2216e-04 - val_loss: 7.3016e-05\n",
            "Epoch 531/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2323e-04 - val_loss: 7.2987e-05\n",
            "Epoch 532/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2291e-04 - val_loss: 6.8217e-05\n",
            "Epoch 533/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1750e-04 - val_loss: 7.0833e-05\n",
            "Epoch 534/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2094e-04 - val_loss: 7.1558e-05\n",
            "Epoch 535/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2157e-04 - val_loss: 6.9057e-05\n",
            "Epoch 536/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1566e-04 - val_loss: 6.9558e-05\n",
            "Epoch 537/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2309e-04 - val_loss: 6.9201e-05\n",
            "Epoch 538/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2343e-04 - val_loss: 6.9698e-05\n",
            "Epoch 539/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2093e-04 - val_loss: 7.1283e-05\n",
            "Epoch 540/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2078e-04 - val_loss: 6.7650e-05\n",
            "Epoch 541/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2190e-04 - val_loss: 7.9630e-05\n",
            "Epoch 542/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2413e-04 - val_loss: 8.0331e-05\n",
            "Epoch 543/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2554e-04 - val_loss: 6.9395e-05\n",
            "Epoch 544/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2490e-04 - val_loss: 7.0263e-05\n",
            "Epoch 545/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2249e-04 - val_loss: 7.4999e-05\n",
            "Epoch 546/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2434e-04 - val_loss: 7.4742e-05\n",
            "Epoch 547/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2175e-04 - val_loss: 7.5380e-05\n",
            "Epoch 548/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2360e-04 - val_loss: 6.5897e-05\n",
            "Epoch 549/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2316e-04 - val_loss: 7.0832e-05\n",
            "Epoch 550/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2121e-04 - val_loss: 7.3452e-05\n",
            "Epoch 551/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2294e-04 - val_loss: 6.7477e-05\n",
            "Epoch 552/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2058e-04 - val_loss: 6.7383e-05\n",
            "Epoch 553/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2459e-04 - val_loss: 6.9059e-05\n",
            "Epoch 554/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2207e-04 - val_loss: 6.7367e-05\n",
            "Epoch 555/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2221e-04 - val_loss: 7.6567e-05\n",
            "Epoch 556/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2502e-04 - val_loss: 7.0524e-05\n",
            "Epoch 557/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2216e-04 - val_loss: 7.0495e-05\n",
            "Epoch 558/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1753e-04 - val_loss: 6.5836e-05\n",
            "Epoch 559/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2466e-04 - val_loss: 6.7391e-05\n",
            "Epoch 560/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2204e-04 - val_loss: 7.3886e-05\n",
            "Epoch 561/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2424e-04 - val_loss: 7.4593e-05\n",
            "Epoch 562/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2385e-04 - val_loss: 7.0202e-05\n",
            "Epoch 563/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1977e-04 - val_loss: 6.9692e-05\n",
            "Epoch 564/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2131e-04 - val_loss: 6.9487e-05\n",
            "Epoch 565/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2057e-04 - val_loss: 8.1448e-05\n",
            "Epoch 566/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2345e-04 - val_loss: 7.2255e-05\n",
            "Epoch 567/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2365e-04 - val_loss: 7.5115e-05\n",
            "Epoch 568/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1997e-04 - val_loss: 7.2908e-05\n",
            "Epoch 569/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1895e-04 - val_loss: 6.9025e-05\n",
            "Epoch 570/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2031e-04 - val_loss: 6.8297e-05\n",
            "Epoch 571/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2294e-04 - val_loss: 7.3597e-05\n",
            "Epoch 572/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2243e-04 - val_loss: 7.7748e-05\n",
            "Epoch 573/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2275e-04 - val_loss: 7.3673e-05\n",
            "Epoch 574/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1874e-04 - val_loss: 7.1928e-05\n",
            "Epoch 575/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2105e-04 - val_loss: 7.1704e-05\n",
            "Epoch 576/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2220e-04 - val_loss: 6.9329e-05\n",
            "Epoch 577/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2628e-04 - val_loss: 7.1517e-05\n",
            "Epoch 578/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2732e-04 - val_loss: 6.9241e-05\n",
            "Epoch 579/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2379e-04 - val_loss: 6.5993e-05\n",
            "Epoch 580/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1960e-04 - val_loss: 7.0185e-05\n",
            "Epoch 581/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2546e-04 - val_loss: 8.0145e-05\n",
            "Epoch 582/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2112e-04 - val_loss: 7.1409e-05\n",
            "Epoch 583/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2087e-04 - val_loss: 7.6578e-05\n",
            "Epoch 584/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1772e-04 - val_loss: 7.5122e-05\n",
            "Epoch 585/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2275e-04 - val_loss: 7.5476e-05\n",
            "Epoch 586/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2485e-04 - val_loss: 7.1378e-05\n",
            "Epoch 587/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2434e-04 - val_loss: 7.5260e-05\n",
            "Epoch 588/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2399e-04 - val_loss: 6.5132e-05\n",
            "Epoch 589/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1851e-04 - val_loss: 7.2578e-05\n",
            "Epoch 590/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2493e-04 - val_loss: 7.1049e-05\n",
            "Epoch 591/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2158e-04 - val_loss: 7.8004e-05\n",
            "Epoch 592/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2044e-04 - val_loss: 6.6641e-05\n",
            "Epoch 593/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1864e-04 - val_loss: 7.0633e-05\n",
            "Epoch 594/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1511e-04 - val_loss: 7.4711e-05\n",
            "Epoch 595/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2136e-04 - val_loss: 7.4192e-05\n",
            "Epoch 596/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1979e-04 - val_loss: 7.1268e-05\n",
            "Epoch 597/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2426e-04 - val_loss: 7.9739e-05\n",
            "Epoch 598/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1961e-04 - val_loss: 7.2107e-05\n",
            "Epoch 599/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2394e-04 - val_loss: 7.1174e-05\n",
            "Epoch 600/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1943e-04 - val_loss: 6.5366e-05\n",
            "Epoch 601/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1701e-04 - val_loss: 7.0058e-05\n",
            "Epoch 602/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2828e-04 - val_loss: 7.0584e-05\n",
            "Epoch 603/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2018e-04 - val_loss: 7.3538e-05\n",
            "Epoch 604/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1851e-04 - val_loss: 7.4188e-05\n",
            "Epoch 605/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1656e-04 - val_loss: 6.5993e-05\n",
            "Epoch 606/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2196e-04 - val_loss: 6.7220e-05\n",
            "Epoch 607/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1882e-04 - val_loss: 7.0747e-05\n",
            "Epoch 608/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2095e-04 - val_loss: 6.5958e-05\n",
            "Epoch 609/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2453e-04 - val_loss: 7.6643e-05\n",
            "Epoch 610/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1969e-04 - val_loss: 7.8233e-05\n",
            "Epoch 611/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2019e-04 - val_loss: 6.7532e-05\n",
            "Epoch 612/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2260e-04 - val_loss: 7.0538e-05\n",
            "Epoch 613/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1912e-04 - val_loss: 6.8257e-05\n",
            "Epoch 614/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2197e-04 - val_loss: 7.3290e-05\n",
            "Epoch 615/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2299e-04 - val_loss: 6.8756e-05\n",
            "Epoch 616/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2426e-04 - val_loss: 6.9766e-05\n",
            "Epoch 617/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2158e-04 - val_loss: 7.2534e-05\n",
            "Epoch 618/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2148e-04 - val_loss: 7.2831e-05\n",
            "Epoch 619/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2152e-04 - val_loss: 7.3853e-05\n",
            "Epoch 620/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1933e-04 - val_loss: 6.7735e-05\n",
            "Epoch 621/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2232e-04 - val_loss: 6.8110e-05\n",
            "Epoch 622/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2514e-04 - val_loss: 6.8474e-05\n",
            "Epoch 623/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1968e-04 - val_loss: 6.8063e-05\n",
            "Epoch 624/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1895e-04 - val_loss: 6.5275e-05\n",
            "Epoch 625/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2297e-04 - val_loss: 7.0680e-05\n",
            "Epoch 626/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2101e-04 - val_loss: 7.6322e-05\n",
            "Epoch 627/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1966e-04 - val_loss: 6.7767e-05\n",
            "Epoch 628/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2596e-04 - val_loss: 6.5501e-05\n",
            "Epoch 629/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1836e-04 - val_loss: 6.8642e-05\n",
            "Epoch 630/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1912e-04 - val_loss: 7.1315e-05\n",
            "Epoch 631/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2304e-04 - val_loss: 7.7422e-05\n",
            "Epoch 632/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2305e-04 - val_loss: 7.4577e-05\n",
            "Epoch 633/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2169e-04 - val_loss: 7.7216e-05\n",
            "Epoch 634/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1867e-04 - val_loss: 7.0811e-05\n",
            "Epoch 635/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1533e-04 - val_loss: 7.4829e-05\n",
            "Epoch 636/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1759e-04 - val_loss: 7.3029e-05\n",
            "Epoch 637/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2053e-04 - val_loss: 7.2876e-05\n",
            "Epoch 638/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1845e-04 - val_loss: 7.3428e-05\n",
            "Epoch 639/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2586e-04 - val_loss: 6.7918e-05\n",
            "Epoch 640/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2645e-04 - val_loss: 7.1548e-05\n",
            "Epoch 641/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1605e-04 - val_loss: 6.4057e-05\n",
            "Epoch 642/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2215e-04 - val_loss: 6.6864e-05\n",
            "Epoch 643/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2093e-04 - val_loss: 6.7857e-05\n",
            "Epoch 644/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1676e-04 - val_loss: 6.9596e-05\n",
            "Epoch 645/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2183e-04 - val_loss: 7.7056e-05\n",
            "Epoch 646/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2127e-04 - val_loss: 7.7975e-05\n",
            "Epoch 647/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1610e-04 - val_loss: 6.9063e-05\n",
            "Epoch 648/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1802e-04 - val_loss: 6.5034e-05\n",
            "Epoch 649/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1757e-04 - val_loss: 6.7103e-05\n",
            "Epoch 650/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1996e-04 - val_loss: 7.2920e-05\n",
            "Epoch 651/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2177e-04 - val_loss: 7.0833e-05\n",
            "Epoch 652/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1670e-04 - val_loss: 7.4287e-05\n",
            "Epoch 653/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2031e-04 - val_loss: 7.0468e-05\n",
            "Epoch 654/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2321e-04 - val_loss: 7.2893e-05\n",
            "Epoch 655/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2306e-04 - val_loss: 7.4423e-05\n",
            "Epoch 656/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2009e-04 - val_loss: 7.3450e-05\n",
            "Epoch 657/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2142e-04 - val_loss: 6.7550e-05\n",
            "Epoch 658/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1846e-04 - val_loss: 6.9446e-05\n",
            "Epoch 659/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2007e-04 - val_loss: 7.0086e-05\n",
            "Epoch 660/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1404e-04 - val_loss: 7.0361e-05\n",
            "Epoch 661/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2225e-04 - val_loss: 7.1474e-05\n",
            "Epoch 662/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2699e-04 - val_loss: 6.9135e-05\n",
            "Epoch 663/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2082e-04 - val_loss: 6.4959e-05\n",
            "Epoch 664/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2038e-04 - val_loss: 7.0914e-05\n",
            "Epoch 665/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1982e-04 - val_loss: 7.0066e-05\n",
            "Epoch 666/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2492e-04 - val_loss: 6.8866e-05\n",
            "Epoch 667/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2271e-04 - val_loss: 6.7851e-05\n",
            "Epoch 668/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1779e-04 - val_loss: 6.9788e-05\n",
            "Epoch 669/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2035e-04 - val_loss: 7.0793e-05\n",
            "Epoch 670/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1913e-04 - val_loss: 6.7162e-05\n",
            "Epoch 671/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1838e-04 - val_loss: 6.4453e-05\n",
            "Epoch 672/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2627e-04 - val_loss: 7.0416e-05\n",
            "Epoch 673/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2164e-04 - val_loss: 6.6314e-05\n",
            "Epoch 674/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1860e-04 - val_loss: 6.7325e-05\n",
            "Epoch 675/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2394e-04 - val_loss: 7.0884e-05\n",
            "Epoch 676/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1825e-04 - val_loss: 6.7450e-05\n",
            "Epoch 677/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2114e-04 - val_loss: 6.6848e-05\n",
            "Epoch 678/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2119e-04 - val_loss: 6.7717e-05\n",
            "Epoch 679/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2219e-04 - val_loss: 8.4021e-05\n",
            "Epoch 680/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1991e-04 - val_loss: 7.1279e-05\n",
            "Epoch 681/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1598e-04 - val_loss: 6.6637e-05\n",
            "Epoch 682/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2249e-04 - val_loss: 7.4074e-05\n",
            "Epoch 683/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1568e-04 - val_loss: 6.5450e-05\n",
            "Epoch 684/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1883e-04 - val_loss: 6.9360e-05\n",
            "Epoch 685/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1697e-04 - val_loss: 7.4713e-05\n",
            "Epoch 686/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2115e-04 - val_loss: 7.2222e-05\n",
            "Epoch 687/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2136e-04 - val_loss: 6.7174e-05\n",
            "Epoch 688/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2134e-04 - val_loss: 7.0188e-05\n",
            "Epoch 689/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1833e-04 - val_loss: 8.0022e-05\n",
            "Epoch 690/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1656e-04 - val_loss: 7.2983e-05\n",
            "Epoch 691/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2014e-04 - val_loss: 7.5133e-05\n",
            "Epoch 692/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2171e-04 - val_loss: 6.5470e-05\n",
            "Epoch 693/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1650e-04 - val_loss: 7.3045e-05\n",
            "Epoch 694/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2257e-04 - val_loss: 7.7016e-05\n",
            "Epoch 695/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2061e-04 - val_loss: 6.3685e-05\n",
            "Epoch 696/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1805e-04 - val_loss: 6.9092e-05\n",
            "Epoch 697/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2239e-04 - val_loss: 6.7490e-05\n",
            "Epoch 698/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2161e-04 - val_loss: 7.1547e-05\n",
            "Epoch 699/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1757e-04 - val_loss: 7.2934e-05\n",
            "Epoch 700/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1956e-04 - val_loss: 7.1860e-05\n",
            "Epoch 701/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1923e-04 - val_loss: 7.1436e-05\n",
            "Epoch 702/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2252e-04 - val_loss: 7.0335e-05\n",
            "Epoch 703/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2005e-04 - val_loss: 6.5611e-05\n",
            "Epoch 704/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1792e-04 - val_loss: 7.0187e-05\n",
            "Epoch 705/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2071e-04 - val_loss: 6.6464e-05\n",
            "Epoch 706/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1983e-04 - val_loss: 6.9165e-05\n",
            "Epoch 707/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2292e-04 - val_loss: 7.0506e-05\n",
            "Epoch 708/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1996e-04 - val_loss: 7.0632e-05\n",
            "Epoch 709/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1766e-04 - val_loss: 7.7695e-05\n",
            "Epoch 710/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1679e-04 - val_loss: 6.6780e-05\n",
            "Epoch 711/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1985e-04 - val_loss: 6.9407e-05\n",
            "Epoch 712/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1945e-04 - val_loss: 6.9046e-05\n",
            "Epoch 713/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2346e-04 - val_loss: 6.7719e-05\n",
            "Epoch 714/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1767e-04 - val_loss: 7.8127e-05\n",
            "Epoch 715/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2030e-04 - val_loss: 7.8729e-05\n",
            "Epoch 716/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2175e-04 - val_loss: 7.4180e-05\n",
            "Epoch 717/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1800e-04 - val_loss: 7.0419e-05\n",
            "Epoch 718/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1857e-04 - val_loss: 6.8789e-05\n",
            "Epoch 719/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1749e-04 - val_loss: 7.1917e-05\n",
            "Epoch 720/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2087e-04 - val_loss: 7.3073e-05\n",
            "Epoch 721/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2097e-04 - val_loss: 6.9568e-05\n",
            "Epoch 722/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2230e-04 - val_loss: 7.1042e-05\n",
            "Epoch 723/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1930e-04 - val_loss: 7.1139e-05\n",
            "Epoch 724/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2015e-04 - val_loss: 7.3853e-05\n",
            "Epoch 725/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1797e-04 - val_loss: 7.4532e-05\n",
            "Epoch 726/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1789e-04 - val_loss: 7.1704e-05\n",
            "Epoch 727/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2238e-04 - val_loss: 7.6087e-05\n",
            "Epoch 728/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2334e-04 - val_loss: 8.5321e-05\n",
            "Epoch 729/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1918e-04 - val_loss: 7.5245e-05\n",
            "Epoch 730/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1495e-04 - val_loss: 6.6203e-05\n",
            "Epoch 731/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1711e-04 - val_loss: 7.0542e-05\n",
            "Epoch 732/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1883e-04 - val_loss: 7.0262e-05\n",
            "Epoch 733/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2101e-04 - val_loss: 6.5944e-05\n",
            "Epoch 734/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2043e-04 - val_loss: 6.1859e-05\n",
            "Epoch 735/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1652e-04 - val_loss: 7.1122e-05\n",
            "Epoch 736/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2032e-04 - val_loss: 7.2641e-05\n",
            "Epoch 737/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1380e-04 - val_loss: 6.6010e-05\n",
            "Epoch 738/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1700e-04 - val_loss: 7.3978e-05\n",
            "Epoch 739/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1926e-04 - val_loss: 6.6305e-05\n",
            "Epoch 740/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1788e-04 - val_loss: 6.8329e-05\n",
            "Epoch 741/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1967e-04 - val_loss: 6.5635e-05\n",
            "Epoch 742/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2155e-04 - val_loss: 7.1503e-05\n",
            "Epoch 743/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1949e-04 - val_loss: 6.7928e-05\n",
            "Epoch 744/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2102e-04 - val_loss: 6.4188e-05\n",
            "Epoch 745/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1626e-04 - val_loss: 6.5314e-05\n",
            "Epoch 746/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2100e-04 - val_loss: 7.2470e-05\n",
            "Epoch 747/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1810e-04 - val_loss: 6.6909e-05\n",
            "Epoch 748/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1632e-04 - val_loss: 6.9499e-05\n",
            "Epoch 749/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1888e-04 - val_loss: 7.1541e-05\n",
            "Epoch 750/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1745e-04 - val_loss: 7.6684e-05\n",
            "Epoch 751/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1425e-04 - val_loss: 6.3565e-05\n",
            "Epoch 752/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1981e-04 - val_loss: 6.9768e-05\n",
            "Epoch 753/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1787e-04 - val_loss: 6.4879e-05\n",
            "Epoch 754/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2171e-04 - val_loss: 7.7011e-05\n",
            "Epoch 755/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1858e-04 - val_loss: 6.6375e-05\n",
            "Epoch 756/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1745e-04 - val_loss: 7.8878e-05\n",
            "Epoch 757/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2084e-04 - val_loss: 7.4120e-05\n",
            "Epoch 758/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2031e-04 - val_loss: 6.8075e-05\n",
            "Epoch 759/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.2275e-04 - val_loss: 7.1046e-05\n",
            "Epoch 760/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1841e-04 - val_loss: 6.7830e-05\n",
            "Epoch 761/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1730e-04 - val_loss: 7.1069e-05\n",
            "Epoch 762/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1731e-04 - val_loss: 7.3513e-05\n",
            "Epoch 763/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1543e-04 - val_loss: 6.6694e-05\n",
            "Epoch 764/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1620e-04 - val_loss: 6.9930e-05\n",
            "Epoch 765/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2118e-04 - val_loss: 6.2853e-05\n",
            "Epoch 766/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2129e-04 - val_loss: 6.6669e-05\n",
            "Epoch 767/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2131e-04 - val_loss: 7.0005e-05\n",
            "Epoch 768/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1826e-04 - val_loss: 6.9389e-05\n",
            "Epoch 769/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1763e-04 - val_loss: 6.9154e-05\n",
            "Epoch 770/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1439e-04 - val_loss: 7.0348e-05\n",
            "Epoch 771/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1885e-04 - val_loss: 7.4608e-05\n",
            "Epoch 772/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2199e-04 - val_loss: 7.9418e-05\n",
            "Epoch 773/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1924e-04 - val_loss: 7.2128e-05\n",
            "Epoch 774/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1829e-04 - val_loss: 8.3774e-05\n",
            "Epoch 775/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1619e-04 - val_loss: 8.1621e-05\n",
            "Epoch 776/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1540e-04 - val_loss: 7.0301e-05\n",
            "Epoch 777/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1726e-04 - val_loss: 6.9355e-05\n",
            "Epoch 778/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1996e-04 - val_loss: 7.4358e-05\n",
            "Epoch 779/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1629e-04 - val_loss: 6.5624e-05\n",
            "Epoch 780/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1643e-04 - val_loss: 7.3640e-05\n",
            "Epoch 781/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1613e-04 - val_loss: 7.2685e-05\n",
            "Epoch 782/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1818e-04 - val_loss: 6.4244e-05\n",
            "Epoch 783/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1893e-04 - val_loss: 8.4903e-05\n",
            "Epoch 784/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1957e-04 - val_loss: 6.6283e-05\n",
            "Epoch 785/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2045e-04 - val_loss: 6.7930e-05\n",
            "Epoch 786/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1855e-04 - val_loss: 6.8600e-05\n",
            "Epoch 787/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2115e-04 - val_loss: 7.3770e-05\n",
            "Epoch 788/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1997e-04 - val_loss: 7.0142e-05\n",
            "Epoch 789/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1855e-04 - val_loss: 6.8672e-05\n",
            "Epoch 790/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1834e-04 - val_loss: 7.0941e-05\n",
            "Epoch 791/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2084e-04 - val_loss: 7.3344e-05\n",
            "Epoch 792/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1570e-04 - val_loss: 7.2531e-05\n",
            "Epoch 793/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1839e-04 - val_loss: 6.6931e-05\n",
            "Epoch 794/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1708e-04 - val_loss: 6.7491e-05\n",
            "Epoch 795/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1717e-04 - val_loss: 7.3346e-05\n",
            "Epoch 796/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1808e-04 - val_loss: 7.1468e-05\n",
            "Epoch 797/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1966e-04 - val_loss: 7.1134e-05\n",
            "Epoch 798/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1826e-04 - val_loss: 6.7434e-05\n",
            "Epoch 799/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2074e-04 - val_loss: 8.5741e-05\n",
            "Epoch 800/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1407e-04 - val_loss: 6.5501e-05\n",
            "Epoch 801/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1267e-04 - val_loss: 6.5774e-05\n",
            "Epoch 802/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1711e-04 - val_loss: 7.4341e-05\n",
            "Epoch 803/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1875e-04 - val_loss: 7.1966e-05\n",
            "Epoch 804/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2139e-04 - val_loss: 7.0522e-05\n",
            "Epoch 805/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1606e-04 - val_loss: 6.9374e-05\n",
            "Epoch 806/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2059e-04 - val_loss: 6.8019e-05\n",
            "Epoch 807/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1881e-04 - val_loss: 7.2206e-05\n",
            "Epoch 808/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1973e-04 - val_loss: 7.4248e-05\n",
            "Epoch 809/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2191e-04 - val_loss: 7.1659e-05\n",
            "Epoch 810/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1260e-04 - val_loss: 6.7603e-05\n",
            "Epoch 811/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2046e-04 - val_loss: 7.1825e-05\n",
            "Epoch 812/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1891e-04 - val_loss: 6.5231e-05\n",
            "Epoch 813/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1993e-04 - val_loss: 6.4176e-05\n",
            "Epoch 814/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2212e-04 - val_loss: 6.5208e-05\n",
            "Epoch 815/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1505e-04 - val_loss: 7.1470e-05\n",
            "Epoch 816/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1687e-04 - val_loss: 6.6084e-05\n",
            "Epoch 817/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1755e-04 - val_loss: 7.7305e-05\n",
            "Epoch 818/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1702e-04 - val_loss: 7.0483e-05\n",
            "Epoch 819/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1993e-04 - val_loss: 7.3945e-05\n",
            "Epoch 820/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1834e-04 - val_loss: 6.8575e-05\n",
            "Epoch 821/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2015e-04 - val_loss: 9.9157e-05\n",
            "Epoch 822/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1814e-04 - val_loss: 6.7710e-05\n",
            "Epoch 823/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1769e-04 - val_loss: 6.5681e-05\n",
            "Epoch 824/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1926e-04 - val_loss: 6.9042e-05\n",
            "Epoch 825/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2211e-04 - val_loss: 8.0350e-05\n",
            "Epoch 826/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1897e-04 - val_loss: 6.5486e-05\n",
            "Epoch 827/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1639e-04 - val_loss: 8.4353e-05\n",
            "Epoch 828/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1818e-04 - val_loss: 6.7839e-05\n",
            "Epoch 829/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1597e-04 - val_loss: 7.5381e-05\n",
            "Epoch 830/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1973e-04 - val_loss: 7.1131e-05\n",
            "Epoch 831/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1981e-04 - val_loss: 6.7534e-05\n",
            "Epoch 832/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1863e-04 - val_loss: 7.0054e-05\n",
            "Epoch 833/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1647e-04 - val_loss: 6.6712e-05\n",
            "Epoch 834/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1593e-04 - val_loss: 7.2417e-05\n",
            "Epoch 835/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2137e-04 - val_loss: 6.9086e-05\n",
            "Epoch 836/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1490e-04 - val_loss: 7.6731e-05\n",
            "Epoch 837/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2087e-04 - val_loss: 7.2336e-05\n",
            "Epoch 838/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1600e-04 - val_loss: 6.7533e-05\n",
            "Epoch 839/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2268e-04 - val_loss: 6.8780e-05\n",
            "Epoch 840/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2035e-04 - val_loss: 6.6829e-05\n",
            "Epoch 841/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1960e-04 - val_loss: 6.9844e-05\n",
            "Epoch 842/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1798e-04 - val_loss: 6.9529e-05\n",
            "Epoch 843/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1616e-04 - val_loss: 6.6296e-05\n",
            "Epoch 844/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1904e-04 - val_loss: 7.4664e-05\n",
            "Epoch 845/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1606e-04 - val_loss: 6.6716e-05\n",
            "Epoch 846/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1903e-04 - val_loss: 7.4612e-05\n",
            "Epoch 847/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1561e-04 - val_loss: 7.4617e-05\n",
            "Epoch 848/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1764e-04 - val_loss: 7.0603e-05\n",
            "Epoch 849/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1697e-04 - val_loss: 6.5301e-05\n",
            "Epoch 850/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2233e-04 - val_loss: 7.7433e-05\n",
            "Epoch 851/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1768e-04 - val_loss: 6.7815e-05\n",
            "Epoch 852/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1579e-04 - val_loss: 6.9330e-05\n",
            "Epoch 853/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1829e-04 - val_loss: 6.7068e-05\n",
            "Epoch 854/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1929e-04 - val_loss: 7.3558e-05\n",
            "Epoch 855/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2134e-04 - val_loss: 6.6834e-05\n",
            "Epoch 856/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1976e-04 - val_loss: 7.2099e-05\n",
            "Epoch 857/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2581e-04 - val_loss: 7.0652e-05\n",
            "Epoch 858/1000\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 2.1317e-04 - val_loss: 6.7766e-05\n",
            "Epoch 859/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1598e-04 - val_loss: 7.5324e-05\n",
            "Epoch 860/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2015e-04 - val_loss: 6.6011e-05\n",
            "Epoch 861/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1613e-04 - val_loss: 7.3742e-05\n",
            "Epoch 862/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1851e-04 - val_loss: 7.5168e-05\n",
            "Epoch 863/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1931e-04 - val_loss: 6.6279e-05\n",
            "Epoch 864/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1454e-04 - val_loss: 8.0313e-05\n",
            "Epoch 865/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1832e-04 - val_loss: 6.5771e-05\n",
            "Epoch 866/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1832e-04 - val_loss: 7.5025e-05\n",
            "Epoch 867/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1451e-04 - val_loss: 7.1739e-05\n",
            "Epoch 868/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1400e-04 - val_loss: 6.7976e-05\n",
            "Epoch 869/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1793e-04 - val_loss: 6.9979e-05\n",
            "Epoch 870/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1579e-04 - val_loss: 6.5876e-05\n",
            "Epoch 871/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1811e-04 - val_loss: 7.1061e-05\n",
            "Epoch 872/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1638e-04 - val_loss: 6.6695e-05\n",
            "Epoch 873/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1957e-04 - val_loss: 6.7274e-05\n",
            "Epoch 874/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1576e-04 - val_loss: 7.6702e-05\n",
            "Epoch 875/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1822e-04 - val_loss: 7.0022e-05\n",
            "Epoch 876/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1762e-04 - val_loss: 6.9747e-05\n",
            "Epoch 877/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2336e-04 - val_loss: 7.3040e-05\n",
            "Epoch 878/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1735e-04 - val_loss: 6.8219e-05\n",
            "Epoch 879/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1707e-04 - val_loss: 6.4895e-05\n",
            "Epoch 880/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1904e-04 - val_loss: 6.7864e-05\n",
            "Epoch 881/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1977e-04 - val_loss: 6.7052e-05\n",
            "Epoch 882/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1880e-04 - val_loss: 7.4295e-05\n",
            "Epoch 883/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1532e-04 - val_loss: 7.6386e-05\n",
            "Epoch 884/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1713e-04 - val_loss: 6.8919e-05\n",
            "Epoch 885/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1870e-04 - val_loss: 7.3766e-05\n",
            "Epoch 886/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2095e-04 - val_loss: 7.0921e-05\n",
            "Epoch 887/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1756e-04 - val_loss: 6.8608e-05\n",
            "Epoch 888/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1631e-04 - val_loss: 6.8791e-05\n",
            "Epoch 889/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1790e-04 - val_loss: 6.2141e-05\n",
            "Epoch 890/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1859e-04 - val_loss: 6.5190e-05\n",
            "Epoch 891/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1469e-04 - val_loss: 7.5849e-05\n",
            "Epoch 892/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1746e-04 - val_loss: 6.9919e-05\n",
            "Epoch 893/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1680e-04 - val_loss: 7.4559e-05\n",
            "Epoch 894/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1918e-04 - val_loss: 6.4861e-05\n",
            "Epoch 895/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1704e-04 - val_loss: 6.4497e-05\n",
            "Epoch 896/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1578e-04 - val_loss: 6.8105e-05\n",
            "Epoch 897/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1746e-04 - val_loss: 6.3062e-05\n",
            "Epoch 898/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1926e-04 - val_loss: 7.1818e-05\n",
            "Epoch 899/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1961e-04 - val_loss: 7.0697e-05\n",
            "Epoch 900/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1919e-04 - val_loss: 6.5946e-05\n",
            "Epoch 901/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2097e-04 - val_loss: 7.2735e-05\n",
            "Epoch 902/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1768e-04 - val_loss: 7.8335e-05\n",
            "Epoch 903/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2080e-04 - val_loss: 6.9329e-05\n",
            "Epoch 904/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1435e-04 - val_loss: 6.5658e-05\n",
            "Epoch 905/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1746e-04 - val_loss: 7.2809e-05\n",
            "Epoch 906/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1485e-04 - val_loss: 7.4657e-05\n",
            "Epoch 907/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1663e-04 - val_loss: 6.7242e-05\n",
            "Epoch 908/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1522e-04 - val_loss: 6.6428e-05\n",
            "Epoch 909/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1551e-04 - val_loss: 7.0629e-05\n",
            "Epoch 910/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1751e-04 - val_loss: 7.5083e-05\n",
            "Epoch 911/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1802e-04 - val_loss: 6.9988e-05\n",
            "Epoch 912/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1973e-04 - val_loss: 7.6908e-05\n",
            "Epoch 913/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1679e-04 - val_loss: 6.9683e-05\n",
            "Epoch 914/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1989e-04 - val_loss: 7.0871e-05\n",
            "Epoch 915/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1694e-04 - val_loss: 6.7777e-05\n",
            "Epoch 916/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1901e-04 - val_loss: 6.7449e-05\n",
            "Epoch 917/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2116e-04 - val_loss: 6.3123e-05\n",
            "Epoch 918/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1686e-04 - val_loss: 7.3613e-05\n",
            "Epoch 919/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1515e-04 - val_loss: 8.1547e-05\n",
            "Epoch 920/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1687e-04 - val_loss: 6.7652e-05\n",
            "Epoch 921/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1765e-04 - val_loss: 6.9991e-05\n",
            "Epoch 922/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2039e-04 - val_loss: 7.1704e-05\n",
            "Epoch 923/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1272e-04 - val_loss: 6.2695e-05\n",
            "Epoch 924/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1459e-04 - val_loss: 6.9472e-05\n",
            "Epoch 925/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1407e-04 - val_loss: 6.5178e-05\n",
            "Epoch 926/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1895e-04 - val_loss: 7.2424e-05\n",
            "Epoch 927/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2039e-04 - val_loss: 7.6492e-05\n",
            "Epoch 928/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1530e-04 - val_loss: 6.7728e-05\n",
            "Epoch 929/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1440e-04 - val_loss: 6.5699e-05\n",
            "Epoch 930/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1718e-04 - val_loss: 6.8833e-05\n",
            "Epoch 931/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1891e-04 - val_loss: 7.1764e-05\n",
            "Epoch 932/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2228e-04 - val_loss: 6.7598e-05\n",
            "Epoch 933/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1633e-04 - val_loss: 7.3760e-05\n",
            "Epoch 934/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1629e-04 - val_loss: 6.8397e-05\n",
            "Epoch 935/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1496e-04 - val_loss: 6.8054e-05\n",
            "Epoch 936/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1939e-04 - val_loss: 6.4223e-05\n",
            "Epoch 937/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1503e-04 - val_loss: 7.0806e-05\n",
            "Epoch 938/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2140e-04 - val_loss: 8.4956e-05\n",
            "Epoch 939/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1697e-04 - val_loss: 6.5002e-05\n",
            "Epoch 940/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1694e-04 - val_loss: 6.6852e-05\n",
            "Epoch 941/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1585e-04 - val_loss: 6.7874e-05\n",
            "Epoch 942/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1590e-04 - val_loss: 7.2222e-05\n",
            "Epoch 943/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1492e-04 - val_loss: 7.6856e-05\n",
            "Epoch 944/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1677e-04 - val_loss: 6.8244e-05\n",
            "Epoch 945/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1748e-04 - val_loss: 7.1430e-05\n",
            "Epoch 946/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1826e-04 - val_loss: 7.1432e-05\n",
            "Epoch 947/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1573e-04 - val_loss: 6.9517e-05\n",
            "Epoch 948/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1676e-04 - val_loss: 6.6478e-05\n",
            "Epoch 949/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1599e-04 - val_loss: 8.0903e-05\n",
            "Epoch 950/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1508e-04 - val_loss: 6.7396e-05\n",
            "Epoch 951/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1717e-04 - val_loss: 7.1286e-05\n",
            "Epoch 952/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1675e-04 - val_loss: 6.4502e-05\n",
            "Epoch 953/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1649e-04 - val_loss: 6.8073e-05\n",
            "Epoch 954/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2145e-04 - val_loss: 6.6205e-05\n",
            "Epoch 955/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1688e-04 - val_loss: 7.4296e-05\n",
            "Epoch 956/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1632e-04 - val_loss: 6.7999e-05\n",
            "Epoch 957/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1914e-04 - val_loss: 6.5607e-05\n",
            "Epoch 958/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1628e-04 - val_loss: 7.3764e-05\n",
            "Epoch 959/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2163e-04 - val_loss: 7.4696e-05\n",
            "Epoch 960/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1827e-04 - val_loss: 6.9964e-05\n",
            "Epoch 961/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1708e-04 - val_loss: 6.8853e-05\n",
            "Epoch 962/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1961e-04 - val_loss: 7.5779e-05\n",
            "Epoch 963/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1552e-04 - val_loss: 7.1826e-05\n",
            "Epoch 964/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1996e-04 - val_loss: 6.1070e-05\n",
            "Epoch 965/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1132e-04 - val_loss: 7.6929e-05\n",
            "Epoch 966/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1932e-04 - val_loss: 8.3145e-05\n",
            "Epoch 967/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1944e-04 - val_loss: 6.5115e-05\n",
            "Epoch 968/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1565e-04 - val_loss: 6.7831e-05\n",
            "Epoch 969/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1529e-04 - val_loss: 6.8058e-05\n",
            "Epoch 970/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2091e-04 - val_loss: 6.6230e-05\n",
            "Epoch 971/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1415e-04 - val_loss: 6.5313e-05\n",
            "Epoch 972/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1410e-04 - val_loss: 7.2423e-05\n",
            "Epoch 973/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2015e-04 - val_loss: 6.9939e-05\n",
            "Epoch 974/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1704e-04 - val_loss: 6.8102e-05\n",
            "Epoch 975/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1399e-04 - val_loss: 6.8533e-05\n",
            "Epoch 976/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1660e-04 - val_loss: 6.7662e-05\n",
            "Epoch 977/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1422e-04 - val_loss: 6.2316e-05\n",
            "Epoch 978/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1848e-04 - val_loss: 6.6931e-05\n",
            "Epoch 979/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1389e-04 - val_loss: 6.6493e-05\n",
            "Epoch 980/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1351e-04 - val_loss: 7.3278e-05\n",
            "Epoch 981/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1618e-04 - val_loss: 6.7670e-05\n",
            "Epoch 982/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1670e-04 - val_loss: 7.3681e-05\n",
            "Epoch 983/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1969e-04 - val_loss: 6.6473e-05\n",
            "Epoch 984/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2309e-04 - val_loss: 6.7994e-05\n",
            "Epoch 985/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1906e-04 - val_loss: 7.8482e-05\n",
            "Epoch 986/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1772e-04 - val_loss: 7.7113e-05\n",
            "Epoch 987/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.2041e-04 - val_loss: 7.0428e-05\n",
            "Epoch 988/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1403e-04 - val_loss: 6.4986e-05\n",
            "Epoch 989/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1588e-04 - val_loss: 7.3542e-05\n",
            "Epoch 990/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1677e-04 - val_loss: 6.4534e-05\n",
            "Epoch 991/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1433e-04 - val_loss: 6.5683e-05\n",
            "Epoch 992/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1560e-04 - val_loss: 7.4781e-05\n",
            "Epoch 993/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1819e-04 - val_loss: 7.0277e-05\n",
            "Epoch 994/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1498e-04 - val_loss: 6.4560e-05\n",
            "Epoch 995/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1840e-04 - val_loss: 7.4249e-05\n",
            "Epoch 996/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1717e-04 - val_loss: 6.6165e-05\n",
            "Epoch 997/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1406e-04 - val_loss: 7.3390e-05\n",
            "Epoch 998/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1656e-04 - val_loss: 6.6590e-05\n",
            "Epoch 999/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1547e-04 - val_loss: 7.5668e-05\n",
            "Epoch 1000/1000\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 2.1456e-04 - val_loss: 6.8775e-05\n",
            "time 1841.04 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())"
      ],
      "metadata": {
        "id": "PUaIWvyKvLCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "epochs = 1000\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(f'Tensorflow Training/Validation Loss for {epochs} Epochs Time: {round(end-start, 2)} sec')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "EWsM9LMjvNuG",
        "outputId": "36d7db21-4f19-4f1e-a2ec-bae6d0efb4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "1X0_VFF6LzLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training, validation and test datasets.\n",
        "# Since it's timeseries we should do it by date.\n",
        "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
        "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
        "\n",
        "df_test = df[df['date_time'] > test_cutoff_date]\n",
        "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
        "df_train = df[df['date_time'] <= val_cutoff_date]\n",
        "\n",
        "#check out the datasets\n",
        "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\n",
        "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\n",
        "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))"
      ],
      "metadata": {
        "id": "efJ2qYEjL1Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Transforming the Dataset for TensorFlow Keras"
      ],
      "metadata": {
        "id": "SDTrhn9KLXb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividing the Dataset into Smaller Dataframes"
      ],
      "metadata": {
        "id": "vMXJDpT4Lag4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Goal of the model:\n",
        "#  Predict Global_active_power at a specified time in the future.\n",
        "#   Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
        "#       We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n",
        "\n",
        "\n",
        "def create_ts_files(dataset, \n",
        "                    start_index, \n",
        "                    end_index, \n",
        "                    history_length, \n",
        "                    step_size, \n",
        "                    target_step, \n",
        "                    num_rows_per_file, \n",
        "                    data_folder):\n",
        "    assert step_size > 0\n",
        "    assert start_index >= 0\n",
        "    \n",
        "    if not os.path.exists(data_folder):\n",
        "        os.makedirs(data_folder)\n",
        "    \n",
        "    time_lags = sorted(range(target_step+1, target_step+history_length+1, step_size), reverse=True)\n",
        "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
        "    start_index = start_index + history_length\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_step\n",
        "    \n",
        "    rng = range(start_index, end_index)\n",
        "    num_rows = len(rng)\n",
        "    num_files = math.ceil(num_rows/num_rows_per_file)\n",
        "    \n",
        "    # for each file.\n",
        "    print(f'Creating {num_files} files.')\n",
        "    for i in range(num_files):\n",
        "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            print(f'{filename}')\n",
        "            \n",
        "        # get the start and end indices.\n",
        "        ind0 = i*num_rows_per_file\n",
        "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
        "        data_list = []\n",
        "        \n",
        "        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n",
        "        for j in range(ind0, ind1):\n",
        "            indices = range(j-1, j-history_length-1, -step_size)\n",
        "            data = dataset[sorted(indices) + [j+target_step]]\n",
        "            \n",
        "            # append data to the list.\n",
        "            data_list.append(data)\n",
        "\n",
        "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
        "        df_ts.to_pickle(filename)\n",
        "            \n",
        "    return len(col_names)-1"
      ],
      "metadata": {
        "id": "LlDFMZy3L5x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "global_active_power = df_train['Global_active_power'].values\n",
        "\n",
        "# Scaled to work with Neural networks.\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).reshape(-1, )\n",
        "\n",
        "history_length = 7*24*60  # The history length in minutes.\n",
        "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
        "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
        "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
        "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
        "\n",
        "# The csv creation returns the number of rows and number of features. We need these values below.\n",
        "num_timesteps = create_ts_files(global_active_power_scaled,\n",
        "                                start_index=0,\n",
        "                                end_index=None,\n",
        "                                history_length=history_length,\n",
        "                                step_size=step_size,\n",
        "                                target_step=target_step,\n",
        "                                num_rows_per_file=128*100,\n",
        "                                data_folder='ts_data')\n",
        "\n",
        "# I found that the easiest way to do time series with tensorflow is by creating pandas files with the lagged time steps (eg. x{t-1}, x{t-2}...) and \n",
        "# the value to predict y = x{t+n}. We tried doing it using TFRecords, but that API is not very intuitive and lacks working examples for time series.\n",
        "# The resulting file using these parameters is over 17GB. If history_length is increased, or  step_size is decreased, it could get much bigger.\n",
        "# Hard to fit into laptop memory, so need to use other means to load the data from the hard drive."
      ],
      "metadata": {
        "id": "sl9Ut2-nL9cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Time Series Object Class"
      ],
      "metadata": {
        "id": "AUUp9UJALdnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# So we can handle loading the data in chunks from the hard drive instead of having to load everything into memory.\n",
        "# \n",
        "# The reason we want to do this is so we can do custom processing on the data that we are feeding into the LSTM.\n",
        "# LSTM requires a certain shape and it is tricky to get it right.\n",
        "#\n",
        "class TimeSeriesLoader:\n",
        "    def __init__(self, ts_folder, filename_format):\n",
        "        self.ts_folder = ts_folder\n",
        "        \n",
        "        # find the number of files.\n",
        "        i = 0\n",
        "        file_found = True\n",
        "        while file_found:\n",
        "            filename = self.ts_folder + '/' + filename_format.format(i)\n",
        "            file_found = os.path.exists(filename)\n",
        "            if file_found:\n",
        "                i += 1\n",
        "                \n",
        "        self.num_files = i\n",
        "        self.files_indices = np.arange(self.num_files)\n",
        "        self.shuffle_chunks()\n",
        "        \n",
        "    def num_chunks(self):\n",
        "        return self.num_files\n",
        "    \n",
        "    def get_chunk(self, idx):\n",
        "        assert (idx >= 0) and (idx < self.num_files)\n",
        "        \n",
        "        ind = self.files_indices[idx]\n",
        "        filename = self.ts_folder + '/' + filename_format.format(ind)\n",
        "        df_ts = pd.read_pickle(filename)\n",
        "        num_records = len(df_ts.index)\n",
        "        \n",
        "        features = df_ts.drop('y', axis=1).values\n",
        "        target = df_ts['y'].values\n",
        "        \n",
        "        # reshape for input into LSTM. Batch major format.\n",
        "        features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
        "        return features_batchmajor, target\n",
        "    \n",
        "    # this shuffles the order the chunks will be outputted from get_chunk.\n",
        "    def shuffle_chunks(self):\n",
        "        np.random.shuffle(self.files_indices)"
      ],
      "metadata": {
        "id": "dSLRSmgaMA6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_folder = 'ts_data'\n",
        "filename_format = 'ts_file{}.pkl'\n",
        "tss = TimeSeriesLoader(ts_folder, filename_format)"
      ],
      "metadata": {
        "id": "uylauUn3MC4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Creating the LSTM Model"
      ],
      "metadata": {
        "id": "XlZBDKvYLk0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8m4If8DKwq-"
      },
      "outputs": [],
      "source": [
        "# Create the Keras model.\n",
        "# Use hyperparameter optimization if you have the time.\n",
        "\n",
        "ts_inputs = tf.keras.Input(shape=(num_timesteps, 1))\n",
        "\n",
        "# units=10 -> The cell and hidden states will be of dimension 10.\n",
        "#             The number of parameters that need to be trained = 4*units*(units+2)\n",
        "x = layers.LSTM(units=10)(ts_inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(1, activation='linear')(x)\n",
        "model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the training configuration.\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['mse'])"
      ],
      "metadata": {
        "id": "At82J3CILSdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "HYZUh5kdMSLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# train in batch sizes of 128.\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 1000\n",
        "NUM_CHUNKS = tss.num_chunks()\n",
        "\n",
        "time_1 = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print('epoch #{}'.format(epoch))\n",
        "    for i in range(NUM_CHUNKS):\n",
        "        X, y = tss.get_chunk(i)\n",
        "        \n",
        "        # model.fit does train the model incrementally. ie. Can call multiple times in batches.\n",
        "        # https://github.com/keras-team/keras/issues/4446\n",
        "        model.fit(x=X, y=y, batch_size=BATCH_SIZE)\n",
        "        \n",
        "    # shuffle the chunks so they're not in the same order next time around.\n",
        "    tss.shuffle_chunks()\n",
        "\n",
        "time_2 = time.time()\n",
        "\n",
        "print(f\"time:{round(time_2 - time_1} secs\", )"
      ],
      "metadata": {
        "id": "PuKz0QnjMUWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model on the validation set.\n",
        "#\n",
        "# Create the validation CSV like we did before with the training.\n",
        "global_active_power_val = df_val['Global_active_power'].values\n",
        "global_active_power_val_scaled = scaler.transform(global_active_power_val.reshape(-1, 1)).reshape(-1, )\n",
        "\n",
        "history_length = 7*24*60  # The history length in minutes.\n",
        "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
        "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
        "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
        "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
        "\n",
        "# The csv creation returns the number of rows and number of features. We need these values below.\n",
        "num_timesteps = create_ts_files(global_active_power_val_scaled,\n",
        "                                start_index=0,\n",
        "                                end_index=None,\n",
        "                                history_length=history_length,\n",
        "                                step_size=step_size,\n",
        "                                target_step=target_step,\n",
        "                                num_rows_per_file=128*100,\n",
        "                                data_folder='ts_val_data')"
      ],
      "metadata": {
        "id": "aKbTaGXlMW-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we assume that the validation dataset can fit into memory we can do this.\n",
        "df_val_ts = pd.read_pickle('ts_val_data/ts_file0.pkl')\n",
        "\n",
        "features = df_val_ts.drop('y', axis=1).values\n",
        "features_arr = np.array(features)\n",
        "\n",
        "# reshape for input into LSTM. Batch major format.\n",
        "num_records = len(df_val_ts.index)\n",
        "features_batchmajor = features_arr.reshape(num_records, -1, 1)\n",
        "\n",
        "\n",
        "y_pred = model.predict(features_batchmajor).reshape(-1, )\n",
        "y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1 ,)\n",
        "\n",
        "y_act = df_val_ts['y'].values\n",
        "y_act = scaler.inverse_transform(y_act.reshape(-1, 1)).reshape(-1 ,)\n",
        "\n",
        "print('validation mean squared error: {}'.format(mean_squared_error(y_act, y_pred)))\n",
        "\n",
        "#baseline\n",
        "y_pred_baseline = df_val_ts['x_lag11'].values\n",
        "y_pred_baseline = scaler.inverse_transform(y_pred_baseline.reshape(-1, 1)).reshape(-1 ,)\n",
        "print('validation baseline mean squared error: {}'.format(mean_squared_error(y_act, y_pred_baseline)))"
      ],
      "metadata": {
        "id": "GXZtTRwlMZPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}