{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488e76a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_forecasting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAE, RMSE, MAPE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelfAttentionPooling\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_forecasting'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# from pytorch_forecasting.metrics import MAE, RMSE, MAPE\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "      \n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "    \n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        \n",
    "        softmax = nn.functional.softmax\n",
    "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "\n",
    "\n",
    "# A custom attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, attention_size, att_hops, non_linearity=\"tanh\"):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.ut_dense =  nn.Sequential(\n",
    "                nn.Linear(hidden_size, attention_size),\n",
    "                nn.Tanh()\n",
    "         )\n",
    "        \n",
    "        self.et_dense = nn.Linear(attention_size, att_hops)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        ##################################################################\n",
    "        # STEP 1 - perform dot product\n",
    "        # of the attention vector and each hidden state\n",
    "        ##################################################################\n",
    "\n",
    "        # inputs is a 3D Tensor: batch, len, hidden_size\n",
    "        # scores is a 2D Tensor: batch, len\n",
    "        ut = self.ut_dense(inputs)\n",
    "\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)\n",
    "\n",
    "        att_scores = self.softmax(torch.permute(et, (0, 2, 1)))\n",
    "\n",
    "        # # re-normalize the masked scores\n",
    "        # _sums = scores.sum(-1, keepdim=True)  # sums per row\n",
    "        # att_scores = scores.div(_sums)  # divide by row sum\n",
    "\n",
    "        ##################################################################\n",
    "        # Step 2 - Weighted sum of hidden states, by the attention scores\n",
    "        ##################################################################\n",
    "        \n",
    "        # print(\"att_scores.shape: \", att_scores.shape, \"inputs.shape\", inputs.shape)\n",
    "\n",
    "        # multiply each hidden state with the attention weights\n",
    "        output = torch.bmm(att_scores, inputs)\n",
    "\n",
    "        return output, att_scores\n",
    "\n",
    "\n",
    "\n",
    "class HierLstmat(nn.Module):\n",
    "\n",
    "    def __init__(self, num_corridor, hidden_size, num_layers, natt_unit, natt_hops, nfc):\n",
    "        super(HierLstmat, self).__init__()\n",
    "\n",
    "        self.input_size = num_corridor\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.sequence_len = 0\n",
    "    \n",
    "        # Initialize LSTM Cell for the first layer\n",
    "        self.lstm_cell_layer_1 = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "\n",
    "        # Initialize LSTM Cell for the second layer\n",
    "        self.lstm_cell_layer_2 = nn.LSTMCell(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        # default maximum upgrade length\n",
    "        self.up_len = 80      \n",
    "\n",
    "        # flattern\n",
    "        self.handle_hops = nn.Sequential(\n",
    "            nn.Flatten()\n",
    "            )\n",
    "\n",
    "        # output layer\n",
    "        self.output_layer =  nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * natt_hops, nfc),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nfc, num_corridor)\n",
    "        )\n",
    "\n",
    "        # attention layer\n",
    "        self.att_encoder = SelfAttention(natt_unit, natt_hops)      \n",
    "        \n",
    "        # attention pooling layer\n",
    "        self.att_pooling = SelfAttentionPooling(self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.sequence_len = x.shape[1]\n",
    "        batch_Size = x.shape[0]\n",
    "        sequence_input = x.transpose(0, 1)\n",
    "\n",
    "        # print(\"sequence_input.device: \", sequence_input.device)\n",
    "      \n",
    "        # batch_size x hidden_size\n",
    "        hidden_state = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
    "        cell_state = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
    "        hidden_state_2 = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
    "        cell_state_2 = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
    "\n",
    "        # print(\"hidden_state.device: \", hidden_state.device)\n",
    "\n",
    "        # weights initialization\n",
    "        torch.nn.init.xavier_normal_(hidden_state)\n",
    "        torch.nn.init.xavier_normal_(cell_state)\n",
    "        torch.nn.init.xavier_normal_(hidden_state_2)\n",
    "        torch.nn.init.xavier_normal_(cell_state_2)\n",
    "\n",
    "        # set upgrade length\n",
    "        up_len = min(self.up_len, math.floor(math.sqrt(self.sequence_len)))\n",
    "        # evenly spaced index\n",
    "        idx = np.linspace(up_len - 1, math.pow(up_len, 2) - 1, num = up_len)\n",
    "        # print(\"sequence index: \", idx)\n",
    "\n",
    "        # initiate pooling hidden_sates an cell states\n",
    "        interverl_hidden_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
    "        interverl_cell_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
    "        outer_hidden_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
    "        outer_cell_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
    "      \n",
    "        # Unfolding LSTM\n",
    "        for i in range(self.sequence_len):\n",
    "        \n",
    "            hidden_state, cell_state = self.lstm_cell_layer_1(sequence_input[i], (hidden_state, cell_state))\n",
    "\n",
    "        if  torch.isnan(interverl_hidden_states).sum() == 0 :\n",
    "            interverl_hidden_states = hidden_state[None, :]\n",
    "            interverl_cell_states = cell_state[None, :]\n",
    "\n",
    "        else:\n",
    "            interverl_hidden_states = torch.cat((interverl_hidden_states, hidden_state[None, :]), 0) # TimeSteps * Batch  * Feature\n",
    "            interverl_cell_states = torch.cat((interverl_cell_states, cell_state[None, :]), 0)       # TimeSteps * Batch  * Feature\n",
    "            # print(\"interverl_hidden_states: \", interverl_hidden_states.shape)\n",
    "\n",
    "        if i in idx or (i == self.sequence_len - 1):\n",
    "\n",
    "            interverl_hidden_states = interverl_hidden_states.transpose(0,1)\n",
    "            interverl_cell_states = interverl_cell_states.transpose(0,1)\n",
    "\n",
    "            # print(\"interverl_hidden_states: \", interverl_hidden_states.shape)\n",
    "            # print(\"interverl_cell_states: \", interverl_cell_states.shape)\n",
    "\n",
    "            layer1_cell_states = torch.cat((interverl_cell_states, cell_state_2[None, :].transpose(0,1)), 1)       # Batch * (TimeSteps + 1) * Feature\n",
    "\n",
    "            layer2_input = self.att_pooling(interverl_hidden_states)   # Batch * Feature\n",
    "            cell_state_2 = self.att_pooling(layer1_cell_states)       # Batch * Feature\n",
    "            \n",
    "            hidden_state_2, cell_state_2 = self.lstm_cell_layer_2(layer2_input, (hidden_state_2, cell_state_2))\n",
    "            interverl_hidden_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
    "            interverl_cell_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
    "\n",
    "            if torch.isnan(outer_hidden_states).sum() == 0:\n",
    "                outer_hidden_states = hidden_state_2[None, :]\n",
    "            else:\n",
    "                outer_hidden_states = torch.cat((outer_hidden_states, hidden_state_2[None, :]))  # # Sequence * Batch  * hiddensize\n",
    "\n",
    "        up_x = torch.transpose(outer_hidden_states, 1, 0)   # size: (batch, Sequence, hiddensize)\n",
    "        # print(\"up_x.shape: \", up_x.shape)   # up_x.shape:  torch.Size([250, 120])\n",
    "        # print(\"reshaped up_x: \", up_x.view(batch_Size, -1, self.hidden_size).shape)   # reshaped up_x:  torch.Size([batch_size, sample_Size, hidden_size])\n",
    "\n",
    "        att_output, att_scores = self.att_encoder(up_x.view(batch_Size, -1, self.hidden_size))\n",
    "        # att_output shape [batch_size, att_hops, LSTM_nhidden]\n",
    "\n",
    "        att_output_flattern = self.handle_hops(att_output)  # [batch_size, att_hops * LSTM_nhidden]\n",
    "        # print(\"att_output_flattern: \", att_output_flattern.shape)\n",
    "\n",
    "        # Last hidden state is passed through a fully connected neural net\n",
    "        output = self.output_layer(att_output_flattern)\n",
    "      \n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_corridor =  num_corridor       # number of corridors also the channel in NTC\n",
    "    hidden_size = 120        # lstm hidden_dim\n",
    "    num_layers = 1          # lstm layers\n",
    "    attention_size = 300     # the hidden_units of attention layer\n",
    "    natt_hops = 2\n",
    "    nfc = 512               # fully connected layer\n",
    "    drop_prob = 0.5         # fully connected layer\n",
    "    batch_size = 50\n",
    "    output = HierLstmat(num_corridor, hidden_size, num_layers, attention_size, natt_hops, nfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e21352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
